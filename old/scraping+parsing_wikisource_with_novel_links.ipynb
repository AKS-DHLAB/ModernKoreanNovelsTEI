{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 위키문헌 작품목록 텍스트 스크래핑\n",
        "- 작성자: [지해인](https://haein.info)\n",
        "- 개별 문서 불러오기 이전까지는 기존 코드를 사용했습니다."
      ],
      "metadata": {
        "id": "A2dhTZMJfjAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 환경 설정"
      ],
      "metadata": {
        "id": "DwJi9_Vyf_fq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hygwvzgqfg6e",
        "outputId": "0247367b-7a2f-4e6a-a2bc-ffa12d293afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mwxml\n",
            "  Downloading mwxml-0.3.6-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from mwxml) (4.25.1)\n",
            "Collecting mwcli>=0.0.2 (from mwxml)\n",
            "  Downloading mwcli-0.0.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting mwtypes>=0.4.0 (from mwxml)\n",
            "  Downloading mwtypes-0.4.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting para>=0.0.1 (from mwxml)\n",
            "  Downloading para-0.0.8-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (0.27.1)\n",
            "Collecting docopt (from mwcli>=0.0.2->mwxml)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonable>=0.3.0 (from mwtypes>=0.4.0->mwxml)\n",
            "  Downloading jsonable-0.3.1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=2.5.1->mwxml) (4.15.0)\n",
            "Downloading mwxml-0.3.6-py2.py3-none-any.whl (33 kB)\n",
            "Downloading mwcli-0.0.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Downloading mwtypes-0.4.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading para-0.0.8-py3-none-any.whl (6.5 kB)\n",
            "Downloading jsonable-0.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=d39387f9f423a50d7b5a1ed05f3b9a832d6beeb794c9d9ea371fd70ef6e25504\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "Successfully built docopt\n",
            "Installing collected packages: para, jsonable, docopt, mwtypes, mwcli, mwxml\n",
            "Successfully installed docopt-0.6.2 jsonable-0.3.1 mwcli-0.0.3 mwtypes-0.4.0 mwxml-0.3.6 para-0.0.8\n",
            "모든 라이브러리가 성공적으로 로드되었습니다!\n",
            "사용 가능한 CPU 코어: 2개\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install mwxml requests tqdm pandas\n",
        "\n",
        "# 기본 라이브러리 임포트\n",
        "import mwxml\n",
        "import bz2\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import urllib.parse\n",
        "import multiprocessing as mp\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "print(\"모든 라이브러리가 성공적으로 로드되었습니다!\")\n",
        "print(f\"사용 가능한 CPU 코어: {mp.cpu_count()}개\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 다운로드"
      ],
      "metadata": {
        "id": "0U0nxEYpf8uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 위키문헌 덤프 다운로드\n",
        "dump_url = \"https://dumps.wikimedia.org/kowikisource/20251001/kowikisource-20251001-pages-articles.xml.bz2\"\n",
        "dump_file = \"kowikisource-20251001-pages-articles.xml.bz2\"\n",
        "\n",
        "if not os.path.exists(dump_file):\n",
        "    print(\" 위키문헌 덤프 다운로드 중... (약 150MB, 시간이 걸릴 수 있습니다)\")\n",
        "    !wget -O {dump_file} {dump_url}\n",
        "    print(\"다운로드 완료!\")\n",
        "else:\n",
        "    print(\"덤프 파일이 이미 존재합니다!\")\n",
        "\n",
        "# 파일 크기 확인\n",
        "file_size = os.path.getsize(dump_file) / (1024 * 1024)  # MB\n",
        "print(f\"파일 크기: {file_size:.1f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gkYrWrafoG7",
        "outputId": "20172bb4-5987-49d9-e435-598f563dfd81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 위키문헌 덤프 다운로드 중... (약 150MB, 시간이 걸릴 수 있습니다)\n",
            "--2025-10-22 03:43:09--  https://dumps.wikimedia.org/kowikisource/20251001/kowikisource-20251001-pages-articles.xml.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.71, 2620:0:861:3:208:80:154:71\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 155556940 (148M) [application/octet-stream]\n",
            "Saving to: ‘kowikisource-20251001-pages-articles.xml.bz2’\n",
            "\n",
            "kowikisource-202510 100%[===================>] 148.35M  3.79MB/s    in 43s     \n",
            "\n",
            "2025-10-22 03:43:53 (3.46 MB/s) - ‘kowikisource-20251001-pages-articles.xml.bz2’ saved [155556940/155556940]\n",
            "\n",
            "다운로드 완료!\n",
            "파일 크기: 148.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API 보강 함수"
      ],
      "metadata": {
        "id": "dvEvHbjFf4ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_categories_from_api(page_title):\n",
        "    \"\"\"\n",
        "    위키문헌 API에서 페이지의 모든 분류를 가져옵니다\n",
        "\n",
        "    이 함수가 중요한 이유:\n",
        "    - XML 덤프에는 기본 분류만 있음\n",
        "    - 템플릿에서 자동 생성되는 분류는 API로만 확인 가능\n",
        "    - 예: '1941년 작품', 'PD-old-50' 등\n",
        "    \"\"\"\n",
        "    try:\n",
        "        api_url = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'categories',\n",
        "            'cllimit': 'max'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(api_url, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return []\n",
        "\n",
        "        categories = pages[page_id].get('categories', [])\n",
        "        category_names = []\n",
        "\n",
        "        for cat in categories:\n",
        "            cat_title = cat['title']\n",
        "            if cat_title.startswith('분류:'):\n",
        "                category_names.append(cat_title[3:])  # '분류:' 제거\n",
        "\n",
        "        return category_names\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"API 분류 조회 오류 ({page_title}): {e}\")\n",
        "        return []\n",
        "\n",
        "def get_year_from_wikidata(page_title):\n",
        "    \"\"\"\n",
        "    위키데이터에서 작품의 발표 연도를 가져옵니다\n",
        "\n",
        "    위키데이터 연동의 장점:\n",
        "    - 구조화된 연도 정보 제공\n",
        "    - 여러 언어판에서 공유되는 정확한 데이터\n",
        "    - 분류 정보와 교차 검증 가능\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1단계: 위키문헌 페이지에서 위키데이터 ID 가져오기\n",
        "        wikisource_api = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'pageprops'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(wikisource_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return None\n",
        "\n",
        "        wikidata_id = pages[page_id].get('pageprops', {}).get('wikibase_item')\n",
        "\n",
        "        if not wikidata_id:\n",
        "            return None\n",
        "\n",
        "        # 2단계: 위키데이터에서 발표일 정보 가져오기\n",
        "        wikidata_api = \"https://www.wikidata.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'format': 'json',\n",
        "            'ids': wikidata_id,\n",
        "            'props': 'claims'\n",
        "        }\n",
        "\n",
        "        response = requests.get(wikidata_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        entity = data.get('entities', {}).get(wikidata_id, {})\n",
        "        claims = entity.get('claims', {})\n",
        "\n",
        "        # 발표 관련 속성들 확인\n",
        "        date_properties = ['P577', 'P571', 'P585']  # 발표일, 시작일, 특정시점\n",
        "\n",
        "        for prop in date_properties:\n",
        "            if prop in claims:\n",
        "                for claim in claims[prop]:\n",
        "                    try:\n",
        "                        time_value = claim['mainsnak']['datavalue']['value']['time']\n",
        "                        # +1941-00-00T00:00:00Z 형태에서 연도 추출\n",
        "                        year = int(time_value[1:5])\n",
        "                        if 1800 <= year <= 2030:  # 유효한 연도 범위\n",
        "                            return year\n",
        "                    except (KeyError, ValueError):\n",
        "                        continue\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"위키데이터 조회 오류 ({page_title}): {e}\")\n",
        "        return None\n",
        "\n",
        "def enhance_with_api(page_data):\n",
        "    \"\"\"\n",
        "    페이지 데이터를 API 정보로 보강합니다\n",
        "\n",
        "    보강 과정:\n",
        "    1. API에서 완전한 분류 정보 가져오기\n",
        "    2. 위키데이터에서 연도 정보 가져오기\n",
        "    3. 분류에서 연도 추출하기\n",
        "    4. 최종 연도 결정 (분류 > 위키데이터 > 덤프)\n",
        "    \"\"\"\n",
        "    title = page_data['title']\n",
        "\n",
        "    # API에서 완전한 분류 정보 가져오기\n",
        "    api_categories = get_categories_from_api(title)\n",
        "\n",
        "    # 위키데이터에서 연도 정보 가져오기\n",
        "    wikidata_year = get_year_from_wikidata(title)\n",
        "\n",
        "    # 기존 데이터 복사\n",
        "    enhanced = page_data.copy()\n",
        "\n",
        "    # 분류 정보 병합 (중복 제거)\n",
        "    all_categories = list(set(page_data.get('categories', []) + api_categories))\n",
        "    enhanced['categories'] = all_categories\n",
        "    enhanced['api_categories'] = api_categories\n",
        "\n",
        "    # 분류에서 연도 추출\n",
        "    year_from_categories = None\n",
        "    year_categories = [cat for cat in all_categories if '년' in cat and '작품' in cat]\n",
        "    if year_categories:\n",
        "        for cat in year_categories:\n",
        "            year_match = re.search(r'(\\d{4})년', cat)\n",
        "            if year_match:\n",
        "                year_from_categories = int(year_match.group(1))\n",
        "                break\n",
        "\n",
        "    # 최종 연도 결정 (우선순위: 분류 > 위키데이터 > 덤프)\n",
        "    enhanced['year'] = (\n",
        "        year_from_categories or\n",
        "        wikidata_year or\n",
        "        page_data.get('year')\n",
        "    )\n",
        "\n",
        "    # 보강 정보 추가\n",
        "    enhanced['year_from_categories'] = year_from_categories\n",
        "    enhanced['year_from_wikidata'] = wikidata_year\n",
        "\n",
        "    return enhanced\n",
        "\n",
        "print(\"API 보강 함수들이 정의되었습니다!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo5Y4WRVft_a",
        "outputId": "f3f32a94-cb41-493d-9241-52881f04ed21"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API 보강 함수들이 정의되었습니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 파싱 함수"
      ],
      "metadata": {
        "id": "iF2hBHdQgKrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_metadata(text):\n",
        "    \"\"\"\n",
        "    위키텍스트에서 메타데이터를 추출합니다\n",
        "\n",
        "    추출하는 정보:\n",
        "    - 저자: [[저자:이름]] 패턴에서\n",
        "    - 분류: [[분류:이름]] 패턴에서\n",
        "    - 작곡가: '작곡' 키워드 주변에서\n",
        "    - 라이선스: {{PD-*}} 템플릿에서\n",
        "    - 언어: '한자', '한글' 키워드에서\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return {\n",
        "            'authors': [],\n",
        "            'categories': [],\n",
        "            'composer': None,\n",
        "            'translator': None,\n",
        "            'year': None,\n",
        "            'license': None,\n",
        "            'language': None\n",
        "        }\n",
        "\n",
        "    # 저자 정보 추출\n",
        "    authors = []\n",
        "    author_patterns = [\n",
        "        r'\\[\\[저자:([^|\\]]+)',  # [[저자:이름]] 또는 [[저자:이름|표시명]]\n",
        "        r'\\|\\s*author\\s*=\\s*\\[\\[저자:([^|\\]]+)',  # author= 매개변수\n",
        "    ]\n",
        "\n",
        "    for pattern in author_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        authors.extend(matches)\n",
        "\n",
        "    # 분류 정보 추출\n",
        "    categories = re.findall(r'\\[\\[분류:([^\\]]+)\\]\\]', text)\n",
        "\n",
        "    # 작곡가 정보 추출\n",
        "    composer = None\n",
        "    composer_patterns = [\n",
        "        r'([^.]+)\\s*작곡',\n",
        "        r'작곡가?\\s*[:=]\\s*([^.\\n]+)'\n",
        "    ]\n",
        "    for pattern in composer_patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        if matches:\n",
        "            composer = matches[0].strip()\n",
        "            break\n",
        "\n",
        "    # 연도 정보 추출\n",
        "    year_matches = re.findall(r'(\\d{4})년', text)\n",
        "    year = None\n",
        "    if year_matches:\n",
        "        year = max(set(year_matches), key=year_matches.count)\n",
        "\n",
        "    # 라이선스 정보 추출\n",
        "    license_info = None\n",
        "    license_patterns = [\n",
        "        r'\\{\\{(PD-[^}]+)\\}\\}',\n",
        "        r'\\{\\{(CC-[^}]+)\\}\\}'\n",
        "    ]\n",
        "    for pattern in license_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            license_info = matches[0]\n",
        "            break\n",
        "\n",
        "    # 언어 정보 추출\n",
        "    language = None\n",
        "    if '한자' in text and '한글' in text:\n",
        "        language = '한자+한글'\n",
        "    elif '한자' in text:\n",
        "        language = '한자'\n",
        "    elif '한글' in text:\n",
        "        language = '한글'\n",
        "\n",
        "    # 중복 제거\n",
        "    authors = list(set([a.strip() for a in authors if a.strip()]))\n",
        "    categories = list(set([c.strip() for c in categories if c.strip()]))\n",
        "\n",
        "    return {\n",
        "        'authors': authors,\n",
        "        'categories': categories,\n",
        "        'composer': composer,\n",
        "        'translator': None,  # 나중에 확장 가능\n",
        "        'year': year,\n",
        "        'license': license_info,\n",
        "        'language': language\n",
        "    }\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    위키텍스트에서 마크업을 제거하고 깔끔한 본문만 추출합니다\n",
        "\n",
        "    제거하는 것들:\n",
        "    - 템플릿: {{...}}\n",
        "    - HTML 태그: <div>, <br> 등\n",
        "    - 위키 링크: [[...]]\n",
        "    - 마크업: ''', ''\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # 템플릿 제거\n",
        "    cleaned = re.sub(r'\\{\\{[^{}]*\\}\\}', '', text)\n",
        "\n",
        "    # 링크에서 텍스트만 추출\n",
        "    cleaned = re.sub(r'\\[\\[[^|\\]]*\\|([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "    cleaned = re.sub(r'\\[\\[([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "\n",
        "    # HTML 태그 제거\n",
        "    cleaned = re.sub(r'<[^>]+>', '', cleaned)\n",
        "\n",
        "    # 위키 마크업 제거\n",
        "    cleaned = re.sub(r\"'''([^']+)'''\", r'\\1', cleaned)  # 굵은 글씨\n",
        "    cleaned = re.sub(r\"''([^']+)''\", r'\\1', cleaned)   # 기울임\n",
        "\n",
        "    # 섹션 헤더 제거\n",
        "    cleaned = re.sub(r'^=+\\s*([^=]+)\\s*=+$', r'\\1', cleaned, flags=re.MULTILINE)\n",
        "\n",
        "    # 여러 공백을 하나로\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_urls(title, authors):\n",
        "    \"\"\"\n",
        "    페이지와 저자의 URL을 생성합니다\n",
        "    \"\"\"\n",
        "    base_url = \"https://ko.wikisource.org/wiki/\"\n",
        "\n",
        "    # 페이지 URL\n",
        "    page_url = base_url + urllib.parse.quote(title.replace(' ', '_'))\n",
        "\n",
        "    # 저자 URL들\n",
        "    author_links = []\n",
        "    for author in authors:\n",
        "        author_url = base_url + urllib.parse.quote(f\"저자:{author}\".replace(' ', '_'))\n",
        "        author_links.append({\n",
        "            'name': author,\n",
        "            'url': author_url\n",
        "        })\n",
        "\n",
        "    return page_url, author_links\n",
        "\n",
        "print(\"텍스트 파싱 함수들이 정의되었습니다!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hCokgZyf7Gq",
        "outputId": "b61ad05a-602c-4a48-edca-752f26adf7bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "텍스트 파싱 함수들이 정의되었습니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 메인 파서"
      ],
      "metadata": {
        "id": "UPgSc7v1gVyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pages_batch(pages_batch):\n",
        "    \"\"\"\n",
        "    페이지 배치를 처리하는 워커 함수\n",
        "    (멀티프로세싱에서 각 코어가 실행)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for page_data in pages_batch:\n",
        "        try:\n",
        "            page_id, title, namespace, redirect, revisions = page_data\n",
        "\n",
        "            if not revisions:\n",
        "                continue\n",
        "\n",
        "            # 최신 리비전 사용\n",
        "            revision = revisions[0]\n",
        "            revision_id, timestamp, username, comment, text, size = revision\n",
        "\n",
        "            # 메타데이터 추출\n",
        "            metadata = extract_metadata(text)\n",
        "\n",
        "            # 본문 정리\n",
        "            clean_content = clean_text(text)\n",
        "\n",
        "            # URL 생성\n",
        "            page_url, author_links = generate_urls(title, metadata['authors'])\n",
        "\n",
        "            # 페이지 데이터 구성\n",
        "            page_result = {\n",
        "                'page_id': page_id,\n",
        "                'title': title,\n",
        "                'url': page_url,\n",
        "                'namespace': namespace,\n",
        "                'redirect': redirect,\n",
        "\n",
        "                # 필수 메타데이터\n",
        "                'authors': metadata['authors'],\n",
        "                'author_links': author_links,\n",
        "                'categories': metadata['categories'],\n",
        "                'content': clean_content,\n",
        "                'raw_content': text,\n",
        "\n",
        "                # 추가 메타데이터\n",
        "                'composer': metadata['composer'],\n",
        "                'translator': metadata['translator'],\n",
        "                'year': metadata['year'],\n",
        "                'license': metadata['license'],\n",
        "                'language': metadata['language'],\n",
        "\n",
        "                # 리비전 정보\n",
        "                'revision_id': revision_id,\n",
        "                'last_modified': str(timestamp) if timestamp else None,\n",
        "                'last_contributor': username,\n",
        "                'size': len(text) if text else 0,\n",
        "                'content_size': len(clean_content)\n",
        "            }\n",
        "\n",
        "            results.append(page_result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"페이지 처리 오류: {e}\")\n",
        "            continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def parse_wikisource(dump_file, limit=None, enable_api=False, batch_size=50):\n",
        "    \"\"\"\n",
        "    위키문헌 덤프를 파싱합니다\n",
        "\n",
        "    Args:\n",
        "        dump_file: 덤프 파일 경로\n",
        "        limit: 처리할 페이지 수 제한 (None이면 전체)\n",
        "        enable_api: API 보강 사용 여부\n",
        "        batch_size: 배치 크기\n",
        "\n",
        "    Returns:\n",
        "        list: 파싱된 페이지 데이터\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # CPU 코어 수 (Colab에서 최대 활용)\n",
        "    max_workers = mp.cpu_count()\n",
        "\n",
        "    print(f\" 위키문헌 파싱 시작!\")\n",
        "    print(f\"   CPU 코어: {max_workers}개 (최대 활용)\")\n",
        "    print(f\"   배치 크기: {batch_size}\")\n",
        "    print(f\"   API 보강: {'사용' if enable_api else '사용 안함'}\")\n",
        "\n",
        "    # 1단계: 덤프에서 데이터 수집\n",
        "    print(\"\\n 1단계: 덤프 데이터 수집\")\n",
        "    pages_batches = []\n",
        "    current_batch = []\n",
        "    total_pages = 0\n",
        "\n",
        "    with bz2.open(dump_file, 'rt', encoding='utf-8') as f:\n",
        "        dump = mwxml.Dump.from_file(f)\n",
        "\n",
        "        for page in tqdm(dump, desc=\"페이지 수집\"):\n",
        "            if limit and total_pages >= limit:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # 리비전 데이터 수집\n",
        "                revisions = []\n",
        "                for revision in page:\n",
        "                    revisions.append((\n",
        "                        revision.id,\n",
        "                        revision.timestamp,\n",
        "                        revision.user.text if revision.user else None,\n",
        "                        revision.comment,\n",
        "                        revision.text,\n",
        "                        revision.bytes\n",
        "                    ))\n",
        "                    break  # 최신 리비전만\n",
        "\n",
        "                page_data = (\n",
        "                    page.id,\n",
        "                    page.title,\n",
        "                    page.namespace,\n",
        "                    str(page.redirect.title) if page.redirect else None,\n",
        "                    revisions\n",
        "                )\n",
        "                current_batch.append(page_data)\n",
        "\n",
        "                if len(current_batch) >= batch_size:\n",
        "                    pages_batches.append(current_batch)\n",
        "                    current_batch = []\n",
        "\n",
        "                total_pages += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if current_batch:\n",
        "            pages_batches.append(current_batch)\n",
        "\n",
        "    print(f\" {total_pages}개 페이지를 {len(pages_batches)}개 배치로 수집\")\n",
        "\n",
        "    # 2단계: 멀티프로세싱으로 병렬 처리\n",
        "    print(\"\\n 2단계: 멀티프로세싱 처리\")\n",
        "    results = []\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # 모든 배치를 병렬로 처리\n",
        "        futures = [executor.submit(process_pages_batch, batch) for batch in pages_batches]\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"배치 처리\"):\n",
        "            try:\n",
        "                batch_results = future.result()\n",
        "                results.extend(batch_results)\n",
        "            except Exception as e:\n",
        "                print(f\"배치 처리 오류: {e}\")\n",
        "\n",
        "    # 3단계: API 보강 (선택적)\n",
        "    if enable_api and results:\n",
        "        print(\"\\n 3단계: API 보강 처리\")\n",
        "        enhanced_results = []\n",
        "\n",
        "        for page_data in tqdm(results, desc=\"API 보강\"):\n",
        "            try:\n",
        "                enhanced = enhance_with_api(page_data)\n",
        "                enhanced_results.append(enhanced)\n",
        "                time.sleep(0.1)  # API 제한 고려\n",
        "            except Exception as e:\n",
        "                print(f\"API 보강 오류 ({page_data.get('title', 'Unknown')}): {e}\")\n",
        "                enhanced_results.append(page_data)\n",
        "\n",
        "        results = enhanced_results\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n파싱 완료!\")\n",
        "    print(f\"  총 시간: {total_time:.1f}초\")\n",
        "    print(f\"  처리 속도: {len(results)/total_time:.1f} 페이지/초\")\n",
        "    print(f\"  총 페이지: {len(results)}개\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"메인 파서가 정의되었습니다!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbbwBTwRgMo8",
        "outputId": "e7d991dd-3ff5-46c6-edc0-6b51181ce45a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메인 파서가 정의되었습니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 대량 파싱 예시"
      ],
      "metadata": {
        "id": "6EZlLpCIgncB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 대량 처리 실행\n",
        "print(\" 실습 3: 대량 처리\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 처리할 페이지 수 설정 (필요에 따라 조정)\n",
        "BULK_LIMIT = 50  # 50개 페이지 처리\n",
        "API_ENHANCEMENT = True  # API 보강 사용 여부\n",
        "\n",
        "print(f\" 설정:\")\n",
        "print(f\"  처리 페이지: {BULK_LIMIT}개\")\n",
        "print(f\"  API 보강: {'사용' if API_ENHANCEMENT else '사용 안함'}\")\n",
        "print(f\"  CPU 활용: {mp.cpu_count()}개 코어 최대 활용\")\n",
        "\n",
        "# 대량 처리 실행\n",
        "bulk_results = parse_wikisource(\n",
        "    dump_file,\n",
        "    limit=BULK_LIMIT,\n",
        "    enable_api=API_ENHANCEMENT,\n",
        "    batch_size=10  # 배치 크기\n",
        ")\n",
        "\n",
        "print(f\"\\n 대량 처리 완료!\")\n",
        "print(f\"처리된 페이지: {len(bulk_results)}개\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWlmjWXcgf4i",
        "outputId": "e0ac2f3d-78a3-44dc-b026-e79ec98cf6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 실습 3: 대량 처리\n",
            "==================================================\n",
            " 설정:\n",
            "  처리 페이지: 50개\n",
            "  API 보강: 사용\n",
            "  CPU 활용: 2개 코어 최대 활용\n",
            " 위키문헌 파싱 시작!\n",
            "   CPU 코어: 2개 (최대 활용)\n",
            "   배치 크기: 10\n",
            "   API 보강: 사용\n",
            "\n",
            " 1단계: 덤프 데이터 수집\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "페이지 수집: 50it [00:00, 220.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 50개 페이지를 5개 배치로 수집\n",
            "\n",
            " 2단계: 멀티프로세싱 처리\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "배치 처리: 100%|██████████| 5/5 [00:11<00:00,  2.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 3단계: API 보강 처리\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "API 보강: 100%|██████████| 50/50 [00:20<00:00,  2.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "파싱 완료!\n",
            "  총 시간: 33.0초\n",
            "  처리 속도: 1.5 페이지/초\n",
            "  총 페이지: 50개\n",
            "\n",
            " 대량 처리 완료!\n",
            "처리된 페이지: 50개\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 개별 문서 불러오기"
      ],
      "metadata": {
        "id": "ZGJr3A6YA2YN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 각 함수의 역할 설명\n",
        "\n",
        "### 1. `extract_page_title(link_url)`: 주소에서 제목만 추출\n",
        "\n",
        "* **역할:** CSV 파일에 있는 긴 **위키문헌 웹 주소**(URL)에서 실제 API 요청에 사용할 수 있는 **깨끗한 작품 제목**만 분리해내는 '주소 정리 담당'입니다.\n",
        "* **작동 방식:** URL에서 `https://ko.wikisource.org/wiki/` 부분을 제거하고, 제목 부분에 있는 URL 인코딩(%)을 원래 문자로 되돌리며, 공백을 나타내는 밑줄(`_`)을 일반 공백으로 바꿉니다.\n",
        "* **예시:** `'https://ko.wikisource.org/wiki/혈의_누'` $\\rightarrow$ **`'혈의 누'`**\n",
        "\n",
        "### 2. `get_wikitext_from_api(page_title)`: 실시간 원본 텍스트 배달\n",
        "\n",
        "* **역할:** 추출한 작품 제목을 가지고 위키문헌 서버에 접속하여, 해당 작품의 **최신 원본 위키텍스트**(Raw Content)를 가져오는 '데이터 수집 담당'입니다.\n",
        "* **작동 방식:** 위키문헌 API에 요청을 보내 해당 페이지의 **가장 최근 수정본**의 내용(`text`)과 수정 시간, 수정한 사용자 등의 **리비전 정보**를 받아옵니다.\n",
        "* **결과 사용:** 이 함수가 가져온 원본 텍스트는 다음 단계인 `extract_metadata`와 `clean_text` 함수의 입력값으로 사용됩니다.\n",
        "\n",
        "### 3. `process_csv_links_with_api(csv_file)`: 전체 작업 흐름 관리 및 통합\n",
        "\n",
        "* **역할:** CSV 파일 로드부터 최종 데이터 보강 및 정리까지 **전체 작업 과정을 총괄**하는 '총 책임자' 함수입니다.\n",
        "* **작동 방식:**\n",
        "    1.  **CSV 읽기:** 입력된 CSV 파일에서 '링크' 열의 주소들을 하나씩 읽습니다.\n",
        "    2.  **순회 및 추출:** 각 링크에 대해 `extract_page_title`을 호출하여 제목을 얻고, `get_wikitext_from_api`를 호출하여 원본 텍스트를 가져옵니다.\n",
        "    3.  **정리 및 구성:** 가져온 원본 텍스트를 이용해 `extract_metadata` (정보 추출)와 `clean_text` (본문 정리) 함수들을 차례로 호출하여 기본적인 데이터를 만듭니다.\n",
        "    4.  **보강 (가장 중요):** `enhance_with_api` 함수를 호출하여 위키데이터 연도, 숨겨진 분류 등 **추가적이고 정확한 정보를 덧붙여(보강하여)** 최종 결과 목록(`final_data`)에 저장합니다.\n",
        "    5.  **반환:** 모든 작품의 처리가 끝나면 정리된 최종 데이터 목록을 반환합니다."
      ],
      "metadata": {
        "id": "_cXPQDm_MGuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikitext_from_api(page_title):\n",
        "    \"\"\"\n",
        "    위키문헌 API를 사용하여 페이지의 위키텍스트(raw content)를 가져옴\n",
        "    \"\"\"\n",
        "    try:\n",
        "        api_url = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'revisions',\n",
        "            'rvprop': 'content|ids|timestamp|user|size|comment',\n",
        "            'rvslots': 'main',\n",
        "            'rvlimit': '1'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(api_url, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return None, None # 텍스트 없음, 리비전 정보 없음\n",
        "\n",
        "        revisions = pages[page_id].get('revisions', [])\n",
        "        if not revisions:\n",
        "            return None, None\n",
        "\n",
        "        revision = revisions[0]\n",
        "        # 'content' 또는 '*' 필드에서 텍스트를 가져옴\n",
        "        text = revision.get('slots', {}).get('main', {}).get('*')\n",
        "\n",
        "        revision_info = {\n",
        "            'revision_id': revision.get('revid'),\n",
        "            'last_modified': revision.get('timestamp'),\n",
        "            'last_contributor': revision.get('user'),\n",
        "            'size': revision.get('size'),\n",
        "            'comment': revision.get('comment')\n",
        "        }\n",
        "\n",
        "        return text, revision_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"API 위키텍스트 조회 오류 ({page_title}): {e}\")\n",
        "        return None, None\n",
        "\n",
        "def extract_page_title(link_url):\n",
        "    \"\"\"\n",
        "    위키문헌 URL에서 페이지 제목을 추출하고 디코딩\n",
        "    예: 'https://ko.wikisource.org/wiki/혈의_누' -> '혈의 누'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # URL 디코딩\n",
        "        decoded_url = urllib.parse.unquote(link_url)\n",
        "        # '/wiki/' 뒤의 부분을 가져와서 밑줄을 공백으로 변환\n",
        "        title_with_underscores = decoded_url.split('/wiki/')[1]\n",
        "        title = title_with_underscores.replace('_', ' ')\n",
        "        return title\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def process_csv_links_with_api(csv_file='한국근대소설_TEI_XML_작품목록.csv'):\n",
        "    \"\"\"\n",
        "    CSV 파일에서 링크를 읽고 API를 통해 위키문헌 데이터를 추출 및 보강합니다.\n",
        "    \"\"\"\n",
        "    print(f\"\\n CSV 파일 로드: {csv_file}\")\n",
        "\n",
        "    # 1. CSV 파일 로드\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\" 오류: '{csv_file}' 파일을 찾을 수 없습니다. 파일을 업로드했는지 확인해주세요.\")\n",
        "        return []\n",
        "\n",
        "    if '링크' not in df.columns:\n",
        "        print(\" 오류: CSV 파일에 '링크' 열이 없습니다. 열 이름을 확인해주세요.\")\n",
        "        return []\n",
        "\n",
        "    valid_links = df['링크'].dropna().astype(str)\n",
        "    print(f\" 총 {len(valid_links)}개의 링크 발견. 데이터 추출 시작...\")\n",
        "\n",
        "    final_data = []\n",
        "\n",
        "    # tqdm을 사용하여 진행 상황 표시\n",
        "    for index, link_url in tqdm(valid_links.items(), total=len(valid_links), desc=\"작품별 API 처리\"):\n",
        "        if not link_url.startswith('https://ko.wikisource.org/wiki/'):\n",
        "            continue\n",
        "\n",
        "        page_title = extract_page_title(link_url)\n",
        "\n",
        "        if not page_title:\n",
        "            continue\n",
        "\n",
        "        # 2. 위키텍스트 및 기본 정보 API로 가져오기\n",
        "        raw_content, revision_info = get_wikitext_from_api(page_title)\n",
        "\n",
        "        if not raw_content:\n",
        "            continue\n",
        "\n",
        "        # 3. 기존 정의된 함수들로 메타데이터 추출 및 본문 정리\n",
        "        metadata = extract_metadata(raw_content)\n",
        "        clean_content = clean_text(raw_content)\n",
        "        page_url, author_links = generate_urls(page_title, metadata['authors'])\n",
        "\n",
        "        # 기본 데이터 구조 생성\n",
        "        page_data = {\n",
        "            'page_id': revision_info.get('revision_id', None),\n",
        "            'title': page_title,\n",
        "            'url': page_url,\n",
        "            'namespace': 0,\n",
        "            'redirect': None,\n",
        "\n",
        "            # 메타데이터\n",
        "            'authors': metadata['authors'],\n",
        "            'author_links': author_links,\n",
        "            'categories': metadata['categories'],\n",
        "            'content': clean_content,\n",
        "            'raw_content': raw_content,\n",
        "\n",
        "            'composer': metadata['composer'],\n",
        "            'translator': metadata['translator'],\n",
        "            'year': metadata['year'], # 덤프 파싱으로 추출된 year (API 보강 후 최종 year 결정)\n",
        "            'license': metadata['license'],\n",
        "            'language': metadata['language'],\n",
        "\n",
        "            # 리비전 정보\n",
        "            'revision_id': revision_info.get('revision_id'),\n",
        "            'last_modified': revision_info.get('last_modified'),\n",
        "            'last_contributor': revision_info.get('last_contributor'),\n",
        "            'size': len(raw_content) if raw_content else 0,\n",
        "            'content_size': len(clean_content)\n",
        "        }\n",
        "\n",
        "        # 4. API 보강 (완전한 분류, 위키데이터 연도 등)\n",
        "        enhanced_data = enhance_with_api(page_data)\n",
        "        final_data.append(enhanced_data)\n",
        "\n",
        "        # API 요청 제한을 위한 대기 (매우 중요)\n",
        "        time.sleep(0.3)\n",
        "\n",
        "    print(f\"\\n 총 {len(final_data)}개의 작품 데이터 추출 및 보강 완료.\")\n",
        "\n",
        "    return final_data\n",
        "\n",
        "print(\"API 호출 및 제목 추출 등을 위한 함수 정의 완료.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6zKcKP1grpS",
        "outputId": "01d58c24-996c-4e50-cf3f-f7f43ce572bc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API 호출 및 제목 추출 등을 위한 함수 정의 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. `dataframe_to_xml(df)`: 데이터를 XML 문서 구조로 변환\n",
        "\n",
        "* **쉽게 말하면:** 정리된 표 형태의 데이터(**DataFrame**)를 **XML**(eXtensible Markup Language)이라는 계층적 문서 형태로 바꿔주는 **'XML 제작 공장'** 입니다. Pandas의 기본 `to_xml()` 기능에 문제가 있을 때 사용되는 **대체 솔루션**입니다.\n",
        "\n",
        "* **핵심 작동 방식:**\n",
        "    1.  **준비:** 데이터를 담을 최상위 태그인 `<works>`를 만듭니다. (XML의 뿌리)\n",
        "    2.  **행(Row) 순회:** DataFrame의 각 행(작품 하나)을 반복하면서, `<work>` 태그를 하나씩 만듭니다.\n",
        "    3.  **태그 배치:**\n",
        "        * `title`, `url`, `year` 같은 단순 값들은 `<title>...</title>` 같은 **일반 태그**로 배치합니다.\n",
        "        * `authors`, `categories`처럼 여러 개가 있는 목록(List)은 `<authors>` 안에 `<author>...</author>`를 여러 개 만드는 **중첩 구조**로 만듭니다.\n",
        "        * `page_id`, `revision_id` 같은 부가 정보는 `<work page_id=\"...\" revision_id=\"...\">`처럼 **속성**(Attribute)으로 추가합니다.\n",
        "    4.  **최종 정리:** 만들어진 복잡한 XML 구조를 사람이 읽기 쉽도록 줄 바꿈과 들여쓰기(`Pretty Print`)를 적용한 후, 파일로 저장할 수 있는 형태로 반환합니다.\n",
        "\n",
        "## 2. `output_data(df, ...)`: 최종 파일을 선택하고 저장\n",
        "\n",
        "* **쉽게 말하면:** 변환된 데이터를 **사용자가 원하는 형식(JSON, CSV, TSV, XML)**으로 파일 저장 작업을 수행하는 **'출력 관리자'** 입니다.\n",
        "\n",
        "* **핵심 작동 방식:**\n",
        "    1.  **형식 결정:** 입력받은 `output_format` 인수에 따라 최종 파일 이름(`extracted_wikisource_modern_novels.xml` 등)을 결정합니다.\n",
        "    2.  **분기 처리:**\n",
        "        * **JSON/CSV/TSV:** Pandas DataFrame이 기본으로 제공하는 `to_json()`, `to_csv()` 함수를 사용하여 파일을 저장합니다.\n",
        "        * **XML:** **`dataframe_to_xml`** 함수를 호출하여 XML 문자열을 받아온 후, 이를 파일에 직접 작성하는 방식으로 저장합니다.\n",
        "    3.  **오류 처리:** 파일을 저장하는 과정에서 오류가 발생하면 사용자에게 알리고, 특히 불안정한 XML 저장이 실패할 경우 **CSV로 대체 저장**하도록 안전장치를 마련합니다."
      ],
      "metadata": {
        "id": "35hV9PP7Mlyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom import minidom\n",
        "\n",
        "def dataframe_to_xml(df):\n",
        "    \"\"\"\n",
        "    Pandas DataFrame을 지정된 포맷의 XML 문자열로 변환합니다.\n",
        "    \"\"\"\n",
        "    root = ET.Element('works')\n",
        "\n",
        "    # NaN 값을 빈 문자열로 처리하여 XML 변환 시 오류를 방지합니다.\n",
        "    df = df.fillna('')\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        work = ET.SubElement(root, 'work')\n",
        "\n",
        "        # 'title', 'url', 'authors', 'year' 등 주요 필드를 태그로 추가\n",
        "        for col in ['title', 'url', 'year', 'license']:\n",
        "            elem = ET.SubElement(work, col)\n",
        "            elem.text = str(row[col])\n",
        "\n",
        "        # authors 목록 처리\n",
        "        authors_elem = ET.SubElement(work, 'authors')\n",
        "        if isinstance(row['authors'], list):\n",
        "             for author_name in row['authors']:\n",
        "                author_elem = ET.SubElement(authors_elem, 'author')\n",
        "                author_elem.text = str(author_name)\n",
        "\n",
        "        # categories 목록 처리\n",
        "        categories_elem = ET.SubElement(work, 'categories')\n",
        "        if isinstance(row['categories'], list):\n",
        "            for cat_name in row['categories']:\n",
        "                cat_elem = ET.SubElement(categories_elem, 'category')\n",
        "                cat_elem.text = str(cat_name)\n",
        "\n",
        "        # 'content' (깔끔한 본문) 처리\n",
        "        content_elem = ET.SubElement(work, 'content')\n",
        "        content_elem.text = row['content']\n",
        "\n",
        "        # 주석 정보 (리비전 정보 등)는 속성으로 추가\n",
        "        work.set('page_id', str(row['page_id']))\n",
        "        work.set('revision_id', str(row['revision_id']))\n",
        "\n",
        "    # Pretty Print (들여쓰기)를 적용하여 사람이 읽기 쉽게 만듭니다.\n",
        "    xml_string = ET.tostring(root, encoding='utf-8')\n",
        "    reparsed = minidom.parseString(xml_string)\n",
        "    return reparsed.toprettyxml(indent=\"  \", encoding='utf-8')\n",
        "\n",
        "print(\"XML 변환 헬퍼 함수 정의 완료.\")\n",
        "\n",
        "def output_data(df, base_filename='extracted_wikisource_modern_novels', output_format='json'):\n",
        "    \"\"\"\n",
        "    DataFrame을 지정된 형식(json, csv, tsv, xml)으로 저장합니다.\n",
        "    \"\"\"\n",
        "    output_format = output_format.lower()\n",
        "    output_filename = f\"{base_filename}.{output_format}\"\n",
        "\n",
        "    print(f\"\\n파일 형식: {output_format.upper()}로 저장 중...\")\n",
        "\n",
        "    try:\n",
        "        if output_format == 'json':\n",
        "            df.to_json(output_filename, orient='records', force_ascii=False, indent=4)\n",
        "        elif output_format == 'csv':\n",
        "            df.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "        elif output_format == 'tsv':\n",
        "            df.to_csv(output_filename, index=False, sep='\\t', encoding='utf-8')\n",
        "        elif output_format == 'xml':\n",
        "            # 수동 XML 변환 함수 사용\n",
        "            xml_output = dataframe_to_xml(df)\n",
        "            with open(output_filename, 'wb') as f:\n",
        "                f.write(xml_output)\n",
        "        else:\n",
        "            print(f\"경고: 지원하지 않는 형식 '{output_format}'. JSON으로 저장합니다.\")\n",
        "            output_filename = f\"{base_filename}.json\"\n",
        "            df.to_json(output_filename, orient='records', force_ascii=False, indent=4)\n",
        "\n",
        "        print(f\" 데이터가 '{output_filename}'으로 저장되었습니다. 다운로드하여 확인하세요.\")\n",
        "    except Exception as e:\n",
        "        print(f\" 데이터 저장 오류 ({output_format}): {e}\")\n",
        "        # XML 저장이 실패하면, CSV로 대체 저장하는 옵션도 고려할 수 있습니다.\n",
        "        if output_format == 'xml':\n",
        "             print(\"대체: XML 저장에 실패하여 CSV로 저장합니다.\")\n",
        "             df.to_csv(f\"{base_filename}_fallback.csv\", index=False, encoding='utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZumYKov0moum",
        "outputId": "dcd9c6cc-cfe3-41c8-be9f-4632ca4d6399"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XML 변환 헬퍼 함수 정의 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 결과 확인 및 DataFrame 변환\n",
        "extracted_works = process_csv_links_with_api()\n",
        "\n",
        "# 결과를 DataFrame으로 변환하여 분석 및 저장\n",
        "if extracted_works:\n",
        "    results_df = pd.DataFrame(extracted_works)\n",
        "    print(\"\\n 추출된 데이터 요약 (상위 5개)\")\n",
        "    print(results_df[['title', 'authors', 'year', 'year_from_wikidata', 'year_from_categories', 'license', 'url', 'content_size']].head())\n",
        "\n",
        "    # 형식 선택: 'json', 'csv', 'tsv', 'xml' 중 하나를 선택하세요.\n",
        "    desired_format = 'xml'\n",
        "\n",
        "    # output_data 함수 실행\n",
        "    output_data(results_df, output_format=desired_format)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJDP0raFBZhq",
        "outputId": "146e7347-bc23-47ba-9b29-62b5c98a4764"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " CSV 파일 로드: 한국근대소설_TEI_XML_작품목록.csv\n",
            " 총 42개의 링크 발견. 데이터 추출 시작...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "작품별 API 처리: 100%|██████████| 42/42 [01:47<00:00,  2.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 총 42개의 작품 데이터 추출 및 보강 완료.\n",
            "\n",
            " 추출된 데이터 요약 (상위 5개)\n",
            "  title      authors  year  year_from_wikidata  year_from_categories  \\\n",
            "0  혈의 누        [이인직]  1906              1906.0                1906.0   \n",
            "1   철세계  [이해조, 쥘 베른]  None                 NaN                   NaN   \n",
            "2   자유종        [이해조]  1910                 NaN                1910.0   \n",
            "3  화의 혈        [이해조]  1911                 NaN                1911.0   \n",
            "4   추월색        [최찬식]  1912                 NaN                1912.0   \n",
            "\n",
            "           license                                                url  \\\n",
            "0       PD-old-100  https://ko.wikisource.org/wiki/%ED%98%88%EC%9D...   \n",
            "1       PD-old-100  https://ko.wikisource.org/wiki/%EC%B2%A0%EC%84...   \n",
            "2  PD-US|1927|1910  https://ko.wikisource.org/wiki/%EC%9E%90%EC%9C...   \n",
            "3        PD-old-70  https://ko.wikisource.org/wiki/%ED%99%94%EC%9D...   \n",
            "4       PD-old-100  https://ko.wikisource.org/wiki/%EC%B6%94%EC%9B...   \n",
            "\n",
            "   content_size  \n",
            "0           347  \n",
            "1         58894  \n",
            "2           303  \n",
            "3         83960  \n",
            "4         61854  \n",
            "\n",
            "파일 형식: XML로 저장 중...\n",
            " 데이터가 'extracted_wikisource_modern_novels.xml'으로 저장되었습니다. 다운로드하여 확인하세요.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 덤프 파일 보강"
      ],
      "metadata": {
        "id": "zgSlBGvUIbEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `find_related_dump_pages`: **대용량 위키문헌 덤프 파일($\\text{kowikisource-...xml.bz2}$)에서 필요한 작품의 모든 텍스트 조각을 찾아 모으는** 역할\n",
        "* 이 함수는 CSV에 있는 **메인 작품**뿐만 아니라, 그 작품의 **장(Chapter)이나 절(Section)이 분리되어 저장된 모든 하위 페이지**를 덤프 전체를 뒤져서 찾아냅니다."
      ],
      "metadata": {
        "id": "QLZb-ciYNFKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import bz2\n",
        "from tqdm import tqdm\n",
        "import mwxml # mwxml 라이브러리는 이미 임포트되어 있어야 합니다.\n",
        "\n",
        "def find_related_dump_pages(dump_file, target_titles):\n",
        "    \"\"\"\n",
        "    덤프 파일에서 목표 작품 제목과 관련된 모든 페이지(본문 및 하위 페이지/장)를 찾습니다.\n",
        "    (하위 페이지 매칭을 최대로 강화)\n",
        "    \"\"\"\n",
        "    related_pages = {}\n",
        "    current_titles = set(target_titles)\n",
        "\n",
        "    print(f\"\\n 덤프 파일에서 {len(current_titles)}개 작품의 관련 페이지 찾기 시작...\")\n",
        "\n",
        "    # 덤프에서 가져올 네임스페이스 (0: 본문, 102: 쪽)\n",
        "    TARGET_NAMESPACES = {0, 102}\n",
        "\n",
        "    try:\n",
        "        with bz2.open(dump_file, 'rt', encoding='utf-8') as f:\n",
        "            dump = mwxml.Dump.from_file(f)\n",
        "\n",
        "            for page in tqdm(dump, desc=\"덤프 페이지 탐색 중\"):\n",
        "                if page.namespace not in TARGET_NAMESPACES or page.redirect:\n",
        "                    continue\n",
        "\n",
        "                text = next(page).text # 최신 리비전 텍스트 추출\n",
        "\n",
        "                is_related = False\n",
        "\n",
        "                for target_title in current_titles:\n",
        "                    # -------------------------------------------------------------\n",
        "                    # 1. Namespace 0 (본문 및 하위 페이지) 매칭\n",
        "                    # -------------------------------------------------------------\n",
        "                    if page.namespace == 0:\n",
        "                        # 1) 정확히 일치 (메인 제목)\n",
        "                        if page.title == target_title:\n",
        "                            is_related = True\n",
        "                            break\n",
        "\n",
        "                        # 2) CSV 제목으로 시작하는 모든 하위 페이지 매칭 (가장 강력한 매칭)\n",
        "                        # 예: '장한몽'과 '장한몽/1장'\n",
        "                        if page.title.startswith(target_title + '/'):\n",
        "                            is_related = True\n",
        "                            break\n",
        "\n",
        "                        # 3) 인코딩된 형식 매칭 (공백 대신 밑줄)\n",
        "                        if page.title.startswith(target_title.replace(' ', '_') + '/'):\n",
        "                            is_related = True\n",
        "                            break\n",
        "\n",
        "                    # -------------------------------------------------------------\n",
        "                    # 2. Namespace 102 (쪽 문서) 매칭\n",
        "                    # -------------------------------------------------------------\n",
        "                    elif page.namespace == 102:\n",
        "                        # 쪽 문서의 베이스 제목이 CSV 작품 제목과 관련 있는지 확인\n",
        "                        match_base = re.search(r'^(?:[^/]+)', page.title.split(':', 1)[-1])\n",
        "                        if match_base:\n",
        "                            base_title = match_base.group(0).strip()\n",
        "                            if target_title in base_title or base_title in target_title:\n",
        "                                is_related = True\n",
        "                                break\n",
        "\n",
        "                if is_related:\n",
        "                    if page.title not in related_pages:\n",
        "                        related_pages[page.title] = {\n",
        "                            'title': page.title,\n",
        "                            'raw_content': text,\n",
        "                            'namespace': page.namespace,\n",
        "                            'revisions': [text]\n",
        "                        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"덤프 파싱 중 치명적인 오류 발생: {e}\")\n",
        "\n",
        "    print(f\" 덤프에서 총 {len(related_pages)}개의 관련 페이지(본문, 하위 장, 쪽 문서) 수집 완료.\")\n",
        "    return related_pages"
      ],
      "metadata": {
        "id": "kWAoth7JII-C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. `sort_wikisource_parts(page_content_list)`: 숫자 순서대로 장(Chapter) 정렬\n",
        "\n",
        "* **역할:** 덤프에서 가져온 작품의 **분리된 텍스트 조각들**($\\text{/1장, /2장, .../10장}$)을 **숫자($\\text{1, 2, 10}$)를 인식**하여 사람이 읽는 순서대로 정확하게 **줄 세우는** 함수입니다.\n",
        "* **핵심:** '10'이 '2'보다 먼저 오지 않도록 **논리적인 순서**를 보장하여 본문 조합 오류를 막습니다.\n",
        "\n",
        "### 2. `process_csv_links_with_dump(csv_file, dump_file)`: 덤프 데이터 통합 및 최종 완성\n",
        "\n",
        "* **역할:** CSV 목록을 기준으로 대용량 덤프($\\text{XML}$)에서 필요한 데이터를 모두 **찾아 모으고, 합치고, 정리**하여 최종 결과물을 만드는 **'총괄 지휘 본부'** 입니다.\n",
        "* **핵심 과정:**\n",
        "    1.  **수집:** $\\text{find\\_related\\_dump\\_pages}$를 호출해 메인 페이지와 모든 **하위 장/쪽 문서**를 가져옵니다.\n",
        "    2.  **조합:** $\\text{sort\\_wikisource\\_parts}$로 순서를 맞춘 후, $\\text{clean\\_text}$로 마크업을 제거하며 **하나의 완성된 본문**으로 합칩니다.\n",
        "    3.  **보강:** $\\text{enhance\\_with\\_api}$로 최신 분류와 위키데이터 연도를 **추가 보강**하여 데이터셋을 완성합니다."
      ],
      "metadata": {
        "id": "K5L5LJZKNTMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_wikisource_parts(page_content_list):\n",
        "    \"\"\"\n",
        "    위키문헌 쪽 문서나 하위 장 제목을 숫자 기반으로 정렬합니다.\n",
        "    (예: .../10장 보다 .../2장이 뒤에 오지 않도록)\n",
        "    \"\"\"\n",
        "    def extract_key(item):\n",
        "        title = item['title']\n",
        "        # 숫자와 알파벳을 포함하는 부분을 추출하고, 나머지는 문자열로 남깁니다.\n",
        "        # 예: '장한몽/1장' -> ('장한몽/', 1, '장')\n",
        "        # 예: '쪽:파일명.pdf/10' -> ('쪽:파일명.pdf/', 10, '')\n",
        "        parts = re.split(r'(\\d+)', title)\n",
        "\n",
        "        # 숫자 부분은 정수로 변환, 나머지 문자열은 그대로 둠\n",
        "        key = []\n",
        "        for part in parts:\n",
        "            if part.isdigit():\n",
        "                key.append(int(part))\n",
        "            elif part:\n",
        "                key.append(part)\n",
        "\n",
        "        return key\n",
        "\n",
        "    return sorted(page_content_list, key=extract_key)\n",
        "\n",
        "\n",
        "def process_csv_links_with_dump(csv_file='한국근대소설_TEI_XML_작품목록.csv', dump_file=\"kowikisource-20251001-pages-articles.xml.bz2\"):\n",
        "    # (CSV 로드 및 target_titles 추출 로직은 이전과 동일)\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file)\n",
        "        target_titles = {extract_page_title(url) for url in df['링크'].dropna() if url.startswith('https://ko.wikisource.org')}\n",
        "        target_titles.discard(None)\n",
        "    except Exception as e:\n",
        "        print(f\" CSV 처리 오류: {e}\")\n",
        "        return []\n",
        "    if not target_titles:\n",
        "        print(\" 오류: 유효한 작품 제목을 추출할 수 없습니다.\")\n",
        "        return []\n",
        "\n",
        "    # 1. 덤프 파일에서 모든 관련 페이지 데이터 수집\n",
        "    dump_data = find_related_dump_pages(dump_file, target_titles)\n",
        "\n",
        "    final_data = []\n",
        "\n",
        "    for page_title in tqdm(target_titles, desc=\"작품별 데이터 조합 및 보강\"):\n",
        "        main_page = None\n",
        "\n",
        "        # 2. 메인 작품 페이지 찾기\n",
        "        for dump_key, data in dump_data.items():\n",
        "            if data['namespace'] == 0 and data['title'] == page_title:\n",
        "                main_page = data\n",
        "                break\n",
        "\n",
        "        if not main_page:\n",
        "            continue\n",
        "\n",
        "        raw_content = main_page['revisions'][0]\n",
        "        metadata = extract_metadata(raw_content)\n",
        "        page_url, author_links = generate_urls(page_title, metadata['authors'])\n",
        "\n",
        "        # 3. 본문 내용 조합 (하위 페이지 + 쪽 문서)\n",
        "        page_content_list = []\n",
        "\n",
        "        # 작품 제목과 관련된 모든 하위/쪽 페이지 수집\n",
        "        for dump_key, data in dump_data.items():\n",
        "            # 메인 페이지는 제외\n",
        "            if data['title'] == page_title:\n",
        "                continue\n",
        "\n",
        "            # 하위 페이지(Namespace 0) 또는 쪽 문서(Namespace 102)인지 확인\n",
        "            is_subpage_or_page = False\n",
        "\n",
        "            # 하위 페이지 형식: '제목/...' 또는 '제목_밑줄/...'\n",
        "            if data['namespace'] == 0 and (data['title'].startswith(page_title + '/') or data['title'].startswith(page_title.replace(' ', '_') + '/')):\n",
        "                is_subpage_or_page = True\n",
        "\n",
        "            # 쪽 문서 형식: '쪽:파일명...'에서 파일명이 제목과 관련될 때\n",
        "            elif data['namespace'] == 102:\n",
        "                match_base = re.search(r'^(?:[^/]+)', data['title'].split(':', 1)[-1])\n",
        "                if match_base and (page_title in data['title'] or match_base.group(0).strip() in page_title):\n",
        "                    is_subpage_or_page = True\n",
        "\n",
        "            if is_subpage_or_page:\n",
        "                page_content_list.append({\n",
        "                    'title': data['title'],\n",
        "                    'content': data['revisions'][0],\n",
        "                    'namespace': data['namespace']\n",
        "                })\n",
        "\n",
        "        combined_content = \"\"\n",
        "\n",
        "        # 3-3. 수집된 모든 페이지 정렬 (숫자 기반 정렬 함수 사용) 및 조합\n",
        "        if page_content_list:\n",
        "            sorted_list = sort_wikisource_parts(page_content_list)\n",
        "\n",
        "            for item in sorted_list:\n",
        "                # 덤프에서 가져온 쪽 문서 텍스트에도 clean_text를 적용하여 불필요한 마크업을 제거\n",
        "                cleaned_page_text = clean_text(item['content'])\n",
        "                combined_content += cleaned_page_text.strip() + \"\\n\\n\"\n",
        "\n",
        "        # 3-4. 내용이 비었으면 메인 페이지의 정리된 내용(목차 등)을 사용\n",
        "        if not combined_content.strip():\n",
        "            # '자유종'의 경우처럼 본문은 없지만 메타데이터는 남아있는 경우를 대비\n",
        "            combined_content = clean_text(raw_content)\n",
        "\n",
        "        # (생략: 최종 데이터 구조 생성 및 API 보강 로직은 이전과 동일)\n",
        "        page_data = {\n",
        "            'title': page_title, 'url': page_url, 'authors': metadata['authors'],\n",
        "            'categories': metadata['categories'], 'content': combined_content.strip(),\n",
        "            'raw_content': raw_content, 'year': metadata['year'], 'license': metadata['license'],\n",
        "            'page_id': None, 'revision_id': None,\n",
        "            'content_size': len(combined_content.strip()) # 정리된 본문의 길이 계산 및 추가\n",
        "        }\n",
        "\n",
        "        enhanced_data = enhance_with_api(page_data)\n",
        "        final_data.append(enhanced_data)\n",
        "\n",
        "    print(f\"\\n 총 {len(final_data)}개의 작품 데이터(덤프 기반) 추출 및 보강 완료.\")\n",
        "\n",
        "    return final_data"
      ],
      "metadata": {
        "id": "Xu9CJAYCE9u7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find_related_dump_pages 함수 정의\n",
        "# 2. sort_wikisource_parts 함수 정의\n",
        "# 3. process_csv_links_with_dump 함수 정의\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4. 실행\n",
        "# ----------------------------------------------------------------------\n",
        "dump_file = \"kowikisource-20251001-pages-articles.xml.bz2\"\n",
        "csv_file = '한국근대소설_TEI_XML_작품목록.csv'\n",
        "\n",
        "print(\"--- 덤프 기반 최종 파싱 시작 ---\")\n",
        "extracted_works = process_csv_links_with_dump(csv_file=csv_file, dump_file=dump_file)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5. 결과 저장\n",
        "# ----------------------------------------------------------------------\n",
        "if extracted_works:\n",
        "    results_df = pd.DataFrame(extracted_works)\n",
        "    print(\"\\n---  추출된 데이터 요약 (상위 5개) ---\")\n",
        "    print(results_df[['title', 'authors', 'year', 'year_from_wikidata', 'year_from_categories', 'license', 'url', 'content_size']].head())\n",
        "\n",
        "    desired_format = 'xml'\n",
        "    # output_data 함수 실행 (XML 변환 로직 포함)\n",
        "    output_data(results_df, output_format=desired_format)\n",
        "else:\n",
        "    print(\"\\n 최종 시도에서도 데이터 추출에 실패했습니다. 덤프 파일과 CSV 파일의 내용/경로를 확인해 주세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1HrRLRAIRI7",
        "outputId": "f74bb85b-b3f6-4c7f-d0e8-78929d68cc9b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 덤프 기반 최종 파싱 시작 ---\n",
            "\n",
            "🔍 덤프 파일에서 42개 작품의 관련 페이지 찾기 시작...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "덤프 페이지 탐색 중: 83815it [01:53, 736.29it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 덤프에서 총 329개의 관련 페이지(본문, 하위 장, 쪽 문서) 수집 완료.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "작품별 데이터 조합 및 보강: 100%|██████████| 42/42 [01:53<00:00,  2.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 총 42개의 작품 데이터(덤프 기반) 추출 및 보강 완료.\n",
            "\n",
            "---  추출된 데이터 요약 (상위 5개) ---\n",
            "          title      authors  year  year_from_wikidata  year_from_categories  \\\n",
            "0           철세계  [이해조, 쥘 베른]  None                 NaN                   NaN   \n",
            "1  어머니와 딸 (강경애)        [강경애]  1931                 NaN                1931.0   \n",
            "2            무정        [이광수]  1918                 NaN                1918.0   \n",
            "3           타락자        [현진건]  1922                 NaN                1922.0   \n",
            "4            도정        [지하련]  1946                 NaN                1946.0   \n",
            "\n",
            "      license                                                url  content_size  \n",
            "0  PD-old-100  https://ko.wikisource.org/wiki/%EC%B2%A0%EC%84...         58894  \n",
            "1   PD-old-50  https://ko.wikisource.org/wiki/%EC%96%B4%EB%A8...         83514  \n",
            "2   PD-old-50  https://ko.wikisource.org/wiki/%EB%AC%B4%EC%A0%95        322845  \n",
            "3   PD-old-50  https://ko.wikisource.org/wiki/%ED%83%80%EB%9D...         48806  \n",
            "4   PD-old-50  https://ko.wikisource.org/wiki/%EB%8F%84%EC%A0%95         18457  \n",
            "\n",
            "파일 형식: XML로 저장 중...\n",
            " 데이터가 'extracted_wikisource_modern_novels.xml'으로 저장되었습니다. 다운로드하여 확인하세요.\n"
          ]
        }
      ]
    }
  ]
}