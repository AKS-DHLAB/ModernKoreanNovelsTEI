{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 위키문헌 작품목록 텍스트 스크래핑\n",
        "- 작성자: [지해인](https://haein.info)"
      ],
      "metadata": {
        "id": "A2dhTZMJfjAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# 필요한 라이브러리 설치\n",
        "!pip install mwxml requests tqdm pandas\n",
        "\n",
        "# 필요한 라이브러리 임포트 (전체 코드에서는 이 셀을 가장 먼저 실행)\n",
        "import mwxml\n",
        "import bz2\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import urllib.parse\n",
        "import multiprocessing as mp\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom import minidom\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 0. 전역 설정 및 변수\n",
        "# -----------------------------------------------------------\n",
        "dump_file = \"kowikisource-20251001-pages-articles.xml.bz2\"\n",
        "csv_file = '한국근대소설_TEI_XML_작품목록.csv'\n",
        "MIN_CONTENT_SIZE_THRESHOLD = 50 # 50바이트 이하인 작품만 덤프 보강 대상으로 선정\n",
        "\n",
        "print(\"모든 라이브러리가 로드되었고 변수가 설정되었습니다.\")\n",
        "# (덤프 파일 다운로드 로직은 생략. 파일이 존재한다고 가정)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1. API 및 유틸리티 함수\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# get_categories_from_api, get_year_from_wikidata, enhance_with_api 함수는 기존 코드를 유지합니다.\n",
        "def get_categories_from_api(page_title):\n",
        "    try:\n",
        "        api_url = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {'action': 'query', 'format': 'json', 'titles': page_title, 'prop': 'categories', 'cllimit': 'max'}\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(api_url, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "        if page_id == '-1': return []\n",
        "        categories = pages[page_id].get('categories', [])\n",
        "        category_names = [cat['title'][3:] for cat in categories if cat['title'].startswith('분류:')]\n",
        "        return category_names\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "def get_year_from_wikidata(page_title):\n",
        "    try:\n",
        "        wikisource_api = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {'action': 'query', 'format': 'json', 'titles': page_title, 'prop': 'pageprops'}\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(wikisource_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "        if page_id == '-1': return None\n",
        "        wikidata_id = pages[page_id].get('pageprops', {}).get('wikibase_item')\n",
        "        if not wikidata_id: return None\n",
        "\n",
        "        wikidata_api = \"https://www.wikidata.org/w/api.php\"\n",
        "        params = {'action': 'wbgetentities', 'format': 'json', 'ids': wikidata_id, 'props': 'claims'}\n",
        "        response = requests.get(wikidata_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "        claims = data.get('entities', {}).get(wikidata_id, {}).get('claims', {})\n",
        "        date_properties = ['P577', 'P571', 'P585']\n",
        "\n",
        "        for prop in date_properties:\n",
        "            if prop in claims:\n",
        "                for claim in claims[prop]:\n",
        "                    try:\n",
        "                        time_value = claim['mainsnak']['datavalue']['value']['time']\n",
        "                        year = int(time_value[1:5])\n",
        "                        if 1800 <= year <= 2030: return year\n",
        "                    except (KeyError, ValueError):\n",
        "                        continue\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def enhance_with_api(page_data):\n",
        "    title = page_data['title']\n",
        "    api_categories = get_categories_from_api(title)\n",
        "    wikidata_year = get_year_from_wikidata(title)\n",
        "    enhanced = page_data.copy()\n",
        "\n",
        "    all_categories = list(set(page_data.get('categories', []) + api_categories))\n",
        "    enhanced['categories'] = all_categories\n",
        "    enhanced['api_categories'] = api_categories\n",
        "\n",
        "    year_from_categories = None\n",
        "    year_categories = [cat for cat in all_categories if '년' in cat and '작품' in cat]\n",
        "    if year_categories:\n",
        "        for cat in year_categories:\n",
        "            year_match = re.search(r'(\\d{4})년', cat)\n",
        "            if year_match:\n",
        "                year_from_categories = int(year_match.group(1)); break\n",
        "\n",
        "    enhanced['year'] = year_from_categories or wikidata_year or page_data.get('year')\n",
        "    enhanced['year_from_categories'] = year_from_categories\n",
        "    enhanced['year_from_wikidata'] = wikidata_year\n",
        "    if enhanced['language'] is None: enhanced['language'] = ''\n",
        "    return enhanced\n",
        "\n",
        "def extract_metadata(text):\n",
        "    if not text:\n",
        "        return {'authors': [], 'categories': [], 'composer': None, 'translator': None,\n",
        "                'year': None, 'license': None, 'language': None}\n",
        "\n",
        "    # (메타데이터 추출 로직 생략: 기존 코드 유지)\n",
        "    authors = []; author_patterns = [r'\\[\\[저자:([^|\\]]+)', r'\\|\\s*author\\s*=\\s*\\[\\[저자:([^|\\]]+)']\n",
        "    for pattern in author_patterns: authors.extend(re.findall(pattern, text))\n",
        "    categories = re.findall(r'\\[\\[분류:([^\\]]+)\\]\\]', text)\n",
        "    composer = None; composer_patterns = [r'([^.]+)\\s*작곡', r'작곡가?\\s*[:=]\\s*([^.\\n]+)']\n",
        "    for pattern in composer_patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        if matches: composer = matches[0].strip(); break\n",
        "    year_matches = re.findall(r'(\\d{4})년', text); year = None\n",
        "    if year_matches: year = max(set(year_matches), key=year_matches.count)\n",
        "    license_info = None; license_patterns = [r'\\{\\{(PD-[^}]+)\\}\\}', r'\\{\\{(CC-[^}]+)\\}\\}']\n",
        "    for pattern in license_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches: license_info = matches[0]; break\n",
        "    language = None\n",
        "    if '한자' in text and '한글' in text: language = '한자+한글'\n",
        "    elif '한자' in text: language = '한자'\n",
        "    elif '한글' in text: language = '한글'\n",
        "    authors = list(set([a.strip() for a in authors if a.strip()]))\n",
        "    categories = list(set([c.strip() for c in categories if c.strip()]))\n",
        "\n",
        "    return {'authors': authors, 'categories': categories, 'composer': composer,\n",
        "            'translator': None, 'year': year, 'license': license_info, 'language': language}\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\" 위키텍스트에서 마크업을 제거하고 단락 구분자(@@PARAGRAPH@@)를 삽입합니다. \"\"\"\n",
        "    if not text: return \"\"\n",
        "    cleaned = re.sub(r'\\{\\{머리말[^{}]*\\}\\}', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
        "    cleaned = re.sub(r'\\{\\{정보[^{}]*\\}\\}', '', cleaned, flags=re.DOTALL | re.IGNORECASE)\n",
        "    cleaned = re.sub(r'\\{\\{[^{}]+\\}\\}', '', cleaned, flags=re.DOTALL)\n",
        "    cleaned = re.sub(r'<[^>]+>', '', cleaned)\n",
        "    cleaned = re.sub(r'\\[\\[(?:파일|File):[^\\]]+\\]\\]', '', cleaned)\n",
        "    cleaned = re.sub(r'\\[\\[분류:[^\\]]+\\]\\]', '', cleaned)\n",
        "    cleaned = re.sub(r'\\n\\n+', '@@PARAGRAPH@@', cleaned)\n",
        "    cleaned = re.sub(r'\\[\\[[^|\\]]*\\|([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "    cleaned = re.sub(r'\\[\\[([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "    cleaned = re.sub(r\"'''?([^']+)'''?\", r'\\1', cleaned)\n",
        "    cleaned = re.sub(r'^=+\\s*([^=]+)\\s*=+$', r'\\1', cleaned, flags=re.MULTILINE)\n",
        "    cleaned = cleaned.replace('\\n', ' ')\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_urls(title, authors):\n",
        "    base_url = \"https://ko.wikisource.org/wiki/\"\n",
        "    page_url = base_url + urllib.parse.quote(title.replace(' ', '_'))\n",
        "    author_links = []\n",
        "    for author in authors:\n",
        "        author_url = base_url + urllib.parse.quote(f\"저자:{author}\".replace(' ', '_'))\n",
        "        author_links.append({'name': author, 'url': author_url})\n",
        "    return page_url, author_links\n",
        "\n",
        "def extract_page_title(link_url):\n",
        "    try:\n",
        "        decoded_url = urllib.parse.unquote(link_url)\n",
        "        title_with_underscores = decoded_url.split('/wiki/')[1]\n",
        "        title = title_with_underscores.replace('_', ' ')\n",
        "        return title\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_wikitext_from_api(page_title):\n",
        "    try:\n",
        "        api_url = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {'action': 'query', 'format': 'json', 'titles': page_title, 'prop': 'revisions',\n",
        "                  'rvprop': 'content|ids|timestamp|user|size|comment', 'rvslots': 'main', 'rvlimit': '1'}\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(api_url, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1': return None, None\n",
        "        revisions = pages[page_id].get('revisions', [])\n",
        "        if not revisions: return None, None\n",
        "        revision = revisions[0]\n",
        "        text = revision.get('slots', {}).get('main', {}).get('*')\n",
        "\n",
        "        revision_info = {'revision_id': revision.get('revid'), 'last_modified': revision.get('timestamp'),\n",
        "                         'last_contributor': revision.get('user'), 'size': revision.get('size'), 'comment': revision.get('comment')}\n",
        "        return text, revision_info\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C6bbFSHm67T",
        "outputId": "619ca55c-1bea-4aae-e315-7435f5de0d8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mwxml\n",
            "  Downloading mwxml-0.3.6-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from mwxml) (4.25.1)\n",
            "Collecting mwcli>=0.0.2 (from mwxml)\n",
            "  Downloading mwcli-0.0.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting mwtypes>=0.4.0 (from mwxml)\n",
            "  Downloading mwtypes-0.4.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting para>=0.0.1 (from mwxml)\n",
            "  Downloading para-0.0.8-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (0.27.1)\n",
            "Collecting docopt (from mwcli>=0.0.2->mwxml)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonable>=0.3.0 (from mwtypes>=0.4.0->mwxml)\n",
            "  Downloading jsonable-0.3.1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=2.5.1->mwxml) (4.15.0)\n",
            "Downloading mwxml-0.3.6-py2.py3-none-any.whl (33 kB)\n",
            "Downloading mwcli-0.0.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Downloading mwtypes-0.4.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading para-0.0.8-py3-none-any.whl (6.5 kB)\n",
            "Downloading jsonable-0.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=a4278ba9ff9a0e9f716f76117dba74cb7281c267f2f81f03533e51d1bf6ecb55\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "Successfully built docopt\n",
            "Installing collected packages: para, jsonable, docopt, mwtypes, mwcli, mwxml\n",
            "Successfully installed docopt-0.6.2 jsonable-0.3.1 mwcli-0.0.3 mwtypes-0.4.0 mwxml-0.3.6 para-0.0.8\n",
            "모든 라이브러리가 로드되었고 변수가 설정되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------\n",
        "# 2. API 기반 메인 파서 (KeyError 해결 포함)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def process_csv_links_with_api(csv_file='한국근대소설_TEI_XML_작품목록.csv'):\n",
        "    \"\"\" CSV 링크를 읽어 API를 통해 모든 작품의 기본 데이터를 추출합니다. \"\"\"\n",
        "    print(f\"\\n CSV 파일 로드: {csv_file}\")\n",
        "    try: df = pd.read_csv(csv_file)\n",
        "    except FileNotFoundError: return []\n",
        "    if '링크' not in df.columns: return []\n",
        "\n",
        "    valid_links = df['링크'].dropna().astype(str)\n",
        "    print(f\" 총 {len(valid_links)}개의 링크 발견. API 추출 시작...\")\n",
        "    final_data = []\n",
        "\n",
        "    for index, link_url in tqdm(valid_links.items(), total=len(valid_links), desc=\"작품별 API 추출\"):\n",
        "        if not link_url.startswith('https://ko.wikisource.org/wiki/'): continue\n",
        "\n",
        "        page_title = extract_page_title(link_url)\n",
        "        if not page_title: continue\n",
        "\n",
        "        raw_content, revision_info = get_wikitext_from_api(page_title)\n",
        "        if not raw_content: continue\n",
        "\n",
        "        metadata = extract_metadata(raw_content)\n",
        "        clean_content = clean_text(raw_content)\n",
        "        page_url, author_links = generate_urls(page_title, metadata['authors'])\n",
        "\n",
        "        page_data = {\n",
        "            'page_id': revision_info.get('revision_id', None), 'title': page_title, 'url': page_url,\n",
        "            'namespace': 0, 'redirect': None, 'authors': metadata['authors'], 'author_links': author_links,\n",
        "            'categories': metadata['categories'], 'content': clean_content, 'raw_content': raw_content,\n",
        "            'composer': metadata['composer'], 'translator': metadata['translator'], 'year': metadata['year'],\n",
        "            'license': metadata['license'], 'language': metadata['language'],\n",
        "            'revision_id': revision_info.get('revid'), 'last_modified': revision_info.get('timestamp'),\n",
        "            'last_contributor': revision_info.get('user'), 'size': len(raw_content) if raw_content else 0,\n",
        "            'content_size': len(clean_content),\n",
        "            #  KeyError 해결: 'source' 필드를 여기서 초기화합니다.\n",
        "            'source': 'API_INITIAL'\n",
        "        }\n",
        "\n",
        "        enhanced_data = enhance_with_api(page_data)\n",
        "        final_data.append(enhanced_data)\n",
        "\n",
        "        time.sleep(0.3)\n",
        "\n",
        "    print(f\"\\n 총 {len(final_data)}개의 작품 데이터 API 추출 완료.\")\n",
        "    return final_data"
      ],
      "metadata": {
        "id": "-kuKg96AqB1y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------\n",
        "# 3. 덤프 수집, 조합, 하이브리드 로직\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def sort_wikisource_parts(page_content_list):\n",
        "    \"\"\" 위키문헌 쪽 문서나 하위 장 제목을 숫자 기반으로 정렬합니다. \"\"\"\n",
        "    def extract_key(item):\n",
        "        title = item['title']\n",
        "        parts = re.split(r'(\\d+)', title)\n",
        "        key = []\n",
        "        for part in parts:\n",
        "            if part.isdigit(): key.append(int(part))\n",
        "            elif part: key.append(part)\n",
        "        return key\n",
        "    return sorted(page_content_list, key=extract_key)\n",
        "\n",
        "def find_related_dump_pages(dump_file, target_titles):\n",
        "    \"\"\" 덤프 파일에서 목표 작품 제목과 관련된 모든 페이지를 찾습니다. \"\"\"\n",
        "    related_pages = {}\n",
        "    current_titles = set(target_titles)\n",
        "    TARGET_NAMESPACES = {0, 102}\n",
        "    print(f\"\\n 덤프 파일에서 {len(current_titles)}개 작품의 관련 페이지 찾기 시작...\")\n",
        "\n",
        "    try:\n",
        "        with bz2.open(dump_file, 'rt', encoding='utf-8') as f:\n",
        "            dump = mwxml.Dump.from_file(f)\n",
        "            for page in tqdm(dump, desc=\"덤프 페이지 탐색 중\"):\n",
        "                if page.namespace not in TARGET_NAMESPACES or page.redirect: continue\n",
        "\n",
        "                text = next(page).text\n",
        "                is_related = False\n",
        "\n",
        "                for target_title in current_titles:\n",
        "                    if page.namespace == 0 and (page.title == target_title or page.title.startswith(target_title + '/') or page.title.startswith(target_title.replace(' ', '_') + '/')):\n",
        "                        is_related = True; break\n",
        "                    elif page.namespace == 102:\n",
        "                        match_base = re.search(r'^(?:[^/]+)', page.title.split(':', 1)[-1])\n",
        "                        if match_base and (target_title in match_base.group(0).strip() or match_base.group(0).strip() in target_title):\n",
        "                            is_related = True; break\n",
        "\n",
        "                if is_related and page.title not in related_pages:\n",
        "                    related_pages[page.title] = {'title': page.title, 'raw_content': text, 'namespace': page.namespace, 'revisions': [text]}\n",
        "    except Exception as e:\n",
        "        print(f\"덤프 파싱 중 치명적인 오류 발생: {e}\")\n",
        "    print(f\" 덤프에서 총 {len(related_pages)}개의 관련 페이지(본문, 하위 장, 쪽 문서) 수집 완료.\")\n",
        "    return related_pages\n",
        "\n",
        "def get_content_from_dump(page_title, dump_data_dict, raw_content):\n",
        "    \"\"\" 단일 작품에 대해 덤프 데이터에서 하위 페이지/쪽 문서를 찾아 본문을 조합합니다. (본문 우선 사용) \"\"\"\n",
        "    page_content_list = []\n",
        "    for dump_key, data in dump_data_dict.items():\n",
        "        if data['title'] == page_title: continue\n",
        "        is_subpage_or_page = False\n",
        "        if data['namespace'] == 0 and (data['title'].startswith(page_title + '/') or data['title'].startswith(page_title.replace(' ', '_') + '/')):\n",
        "            is_subpage_or_page = True\n",
        "        elif data['namespace'] == 102:\n",
        "            match_base = re.search(r'^(?:[^/]+)', data['title'].split(':', 1)[-1])\n",
        "            if match_base and (page_title in match_base.group(0).strip() or match_base.group(0).strip() in page_title):\n",
        "                is_subpage_or_page = True\n",
        "\n",
        "        if is_subpage_or_page:\n",
        "            page_content_list.append({'title': data['title'], 'content': data['revisions'][0], 'namespace': data['namespace']})\n",
        "\n",
        "    combined_content = \"\"\n",
        "    if page_content_list:\n",
        "        sorted_list = sort_wikisource_parts(page_content_list)\n",
        "        for item in sorted_list:\n",
        "            cleaned_page_text = clean_text(item['content'])\n",
        "            combined_content += cleaned_page_text.strip() + \"\\n\\n\"\n",
        "        final_content = combined_content.strip()\n",
        "    else:\n",
        "        final_content = clean_text(raw_content).strip() # 덤프에서 본문을 못 찾으면 메인 페이지 정리본 사용\n",
        "\n",
        "    return final_content\n",
        "\n",
        "def process_csv_links_hybrid(csv_file, dump_file, min_content_size=MIN_CONTENT_SIZE_THRESHOLD):\n",
        "    \"\"\" API를 기본으로 사용하고, 본문 크기가 min_content_size 이하인 작품만 덤프로 보강합니다. \"\"\"\n",
        "    print(\"\\n--- 1단계: API 기반 기본 추출 및 보강 ---\")\n",
        "    api_results = process_csv_links_with_api(csv_file=csv_file)\n",
        "    if not api_results: return []\n",
        "\n",
        "    # 2. 덤프 보강 대상 작품 목록 준비\n",
        "    dump_target_titles = set()\n",
        "    for item in api_results:\n",
        "        # 본문 길이가 min_content_size 이하인 작품만 보강 대상으로 선정\n",
        "        if item.get('content_size', 0) <= min_content_size:\n",
        "            dump_target_titles.add(item['title'])\n",
        "\n",
        "    print(f\"\\n--- 2단계: 덤프 보강 대상 확인 ---\")\n",
        "    print(f\"총 {len(api_results)}개 작품 중, {min_content_size}바이트 이하인 {len(dump_target_titles)}개 작품을 덤프 보강 대상으로 선정했습니다.\")\n",
        "\n",
        "    if not dump_target_titles: return api_results\n",
        "\n",
        "    # 3. 덤프 파일에서 보강 대상 작품의 모든 관련 페이지 수집\n",
        "    print(f\"\\n--- 3단계: 덤프 데이터 수집 (보강 대상만) ---\")\n",
        "    dump_data_dict = find_related_dump_pages(dump_file, dump_target_titles)\n",
        "\n",
        "    # 4. API 결과에 덤프 본문 적용\n",
        "    print(f\"\\n--- 4단계: 덤프 본문 적용 및 최종 데이터 완성 ---\")\n",
        "    final_results = []\n",
        "\n",
        "    for item in tqdm(api_results, desc=\"덤프 보강 적용\"):\n",
        "        if item['title'] in dump_target_titles:\n",
        "            new_content = get_content_from_dump(item['title'], dump_data_dict, item['raw_content'])\n",
        "\n",
        "            # 덤프 본문이 기존 API 본문보다 길다면 (진짜 본문을 찾았다면) 덮어씁니다.\n",
        "            if len(new_content) > item.get('content_size', 0):\n",
        "                item['content'] = new_content\n",
        "                item['content_size'] = len(new_content)\n",
        "                item['source'] = 'API_DUMP_HYBRID'\n",
        "            else:\n",
        "                item['source'] = 'API_ONLY_SHORT'\n",
        "        else:\n",
        "            item['source'] = 'API_ONLY_LONG'\n",
        "\n",
        "        final_results.append(item)\n",
        "\n",
        "    print(f\"\\n총 {len(final_results)}개의 작품 데이터 하이브리드 추출 완료.\")\n",
        "    return final_results"
      ],
      "metadata": {
        "id": "r2yWefqwqD8K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------\n",
        "# 4. XML 출력 함수\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def dataframe_to_xml(df):\n",
        "    \"\"\" Pandas DataFrame을 XML 문자열로 변환합니다. (단락 구조 보존 및 안전한 키 접근) \"\"\"\n",
        "    root = ET.Element('works')\n",
        "    df = df.fillna('')\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        work = ET.SubElement(root, 'work')\n",
        "\n",
        "        for col in ['title', 'url', 'year', 'license', 'language']:\n",
        "            elem = ET.SubElement(work, col)\n",
        "            elem.text = str(row.get(col, ''))\n",
        "\n",
        "        authors_elem = ET.SubElement(work, 'authors')\n",
        "        if isinstance(row.get('authors'), list):\n",
        "             for author_name in row['authors']:\n",
        "                author_elem = ET.SubElement(authors_elem, 'author'); author_elem.text = str(author_name)\n",
        "\n",
        "        categories_elem = ET.SubElement(work, 'categories')\n",
        "        if isinstance(row.get('categories'), list):\n",
        "            for cat_name in row['categories']:\n",
        "                cat_elem = ET.SubElement(categories_elem, 'category'); cat_elem.text = str(cat_name)\n",
        "\n",
        "        # 콘텐츠 구조화 (@@PARAGRAPH@@ 사용)\n",
        "        content_text = row.get('content', '')\n",
        "        content_root = ET.SubElement(work, 'content')\n",
        "\n",
        "        blocks = content_text.split('@@PARAGRAPH@@')\n",
        "\n",
        "        for block in blocks:\n",
        "            block_text = block.strip()\n",
        "            if not block_text: continue\n",
        "\n",
        "            if block_text.startswith('='):\n",
        "                header = ET.SubElement(content_root, 'heading')\n",
        "                header.text = block_text\n",
        "            elif block_text.startswith(('*', '-')):\n",
        "                 list_item = ET.SubElement(content_root, 'list_item')\n",
        "                 list_item.text = block_text\n",
        "            else:\n",
        "                paragraph = ET.SubElement(content_root, 'paragraph')\n",
        "                paragraph.text = block_text\n",
        "\n",
        "        work.set('page_id', str(row.get('page_id', '')))\n",
        "        work.set('revision_id', str(row.get('revision_id', '')))\n",
        "\n",
        "    xml_string = ET.tostring(root, encoding='utf-8')\n",
        "    reparsed = minidom.parseString(xml_string)\n",
        "    return reparsed.toprettyxml(indent=\"  \", encoding='utf-8')\n",
        "\n",
        "\n",
        "def output_data(df, base_filename='extracted_wikisource_modern_novels', output_format='json'):\n",
        "    \"\"\" DataFrame을 지정된 형식(json, csv, tsv, xml)으로 저장합니다. \"\"\"\n",
        "    output_format = output_format.lower()\n",
        "    output_filename = f\"{base_filename}.{output_format}\"\n",
        "\n",
        "    print(f\"\\n파일 형식: {output_format.upper()}로 저장 중...\")\n",
        "\n",
        "    try:\n",
        "        if output_format == 'xml':\n",
        "            xml_output = dataframe_to_xml(df)\n",
        "            with open(output_filename, 'wb') as f:\n",
        "                f.write(xml_output)\n",
        "        elif output_format == 'json':\n",
        "            df.to_json(output_filename, orient='records', force_ascii=False, indent=4)\n",
        "        elif output_format == 'csv':\n",
        "            df.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "        elif output_format == 'tsv':\n",
        "            df.to_csv(output_filename, index=False, sep='\\t', encoding='utf-8')\n",
        "        else:\n",
        "             print(f\"경고: 지원하지 않는 형식 '{output_format}'. JSON으로 저장합니다.\")\n",
        "\n",
        "        print(f\" 데이터가 '{output_filename}'으로 저장되었습니다. 다운로드하여 확인하세요.\")\n",
        "    except Exception as e:\n",
        "        print(f\" 데이터 저장 오류 ({output_format}): {e}\")\n",
        "        if output_format == 'xml':\n",
        "             print(\"대체: XML 저장에 실패하여 CSV로 저장합니다.\")\n",
        "             df.to_csv(f\"{base_filename}_fallback.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 5. 최종 실행 (이 코드를 Colab에 붙여넣고 실행)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "print(\"--- 하이브리드 파싱 시작 ---\")\n",
        "\n",
        "# min_content_size=50: API로 추출한 본문이 50바이트 이하인 작품만 덤프 보강\n",
        "extracted_works_hybrid = process_csv_links_hybrid(\n",
        "    csv_file=csv_file,\n",
        "    dump_file=dump_file,\n",
        "    min_content_size=50\n",
        ")\n",
        "\n",
        "if extracted_works_hybrid:\n",
        "    results_df = pd.DataFrame(extracted_works_hybrid)\n",
        "    print(\"\\n---  최종 하이브리드 데이터 요약 (상위 5개) ---\")\n",
        "    print(results_df[['title', 'authors', 'year', 'content_size', 'source']].head())\n",
        "\n",
        "    desired_format = 'xml'\n",
        "    output_data(results_df, output_format=desired_format)\n",
        "else:\n",
        "    print(\"\\n 데이터 추출에 실패했습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjvfXJ97qGbE",
        "outputId": "4aab85e0-56f7-47d5-e318-705b74203bed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 하이브리드 파싱 시작 ---\n",
            "\n",
            "--- 1단계: API 기반 기본 추출 및 보강 ---\n",
            "\n",
            " CSV 파일 로드: 한국근대소설_TEI_XML_작품목록.csv\n",
            " 총 311개의 링크 발견. API 추출 시작...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "작품별 API 추출: 100%|██████████| 311/311 [04:33<00:00,  1.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 총 311개의 작품 데이터 API 추출 완료.\n",
            "\n",
            "--- 2단계: 덤프 보강 대상 확인 ---\n",
            "총 311개 작품 중, 50바이트 이하인 0개 작품을 덤프 보강 대상으로 선정했습니다.\n",
            "\n",
            "---  최종 하이브리드 데이터 요약 (상위 5개) ---\n",
            "  title      authors  year  content_size       source\n",
            "0  혈의 누        [이인직]  1906           389  API_INITIAL\n",
            "1   철세계  [쥘 베른, 이해조]  None         67624  API_INITIAL\n",
            "2   자유종        [이해조]  1910           280  API_INITIAL\n",
            "3  화의 혈        [이해조]  1911         85899  API_INITIAL\n",
            "4   추월색        [최찬식]  1912         66870  API_INITIAL\n",
            "\n",
            "파일 형식: XML로 저장 중...\n",
            " 데이터가 'extracted_wikisource_modern_novels.xml'으로 저장되었습니다. 다운로드하여 확인하세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1. 개별 파일 저장 함수\n",
        "# -----------------------------------------------------------\n",
        "def split_and_save_by_work(df, output_dir='individual_works', output_format='xml'):\n",
        "    \"\"\"\n",
        "    DataFrame의 각 행(작품)을 개별 파일로 분리하여 저장합니다.\n",
        "    (dataframe_to_xml 함수는 메모리에 정의되어 있어야 함)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. 출력 디렉토리 생성\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "        print(f\"출력 디렉토리 생성: {output_dir}/\")\n",
        "    else:\n",
        "        print(f\"기존 디렉토리 사용: {output_dir}/\")\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    # 2. 각 작품을 순회하며 저장\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"개별 작품 ({output_format.upper()}) 저장\"):\n",
        "        try:\n",
        "            # 파일 이름에 사용할 안전한 제목 생성\n",
        "            title = str(row['title'])\n",
        "            # 파일명 금지 문자를 제거하고 공백을 밑줄로 변경\n",
        "            safe_title = re.sub(r'[\\\\/:*?\"<>|]+', '', title).replace(' ', '_')\n",
        "\n",
        "            output_filename = os.path.join(output_dir, f\"{safe_title}.{output_format}\")\n",
        "\n",
        "            # 현재 작품(행)만 담은 임시 DataFrame 생성\n",
        "            work_df = pd.DataFrame([row])\n",
        "\n",
        "            if output_format == 'xml':\n",
        "                # dataframe_to_xml 함수 사용\n",
        "                xml_output = dataframe_to_xml(work_df)\n",
        "                with open(output_filename, 'wb') as f:\n",
        "                    f.write(xml_output)\n",
        "\n",
        "            elif output_format == 'json':\n",
        "                work_df.to_json(output_filename, orient='records', force_ascii=False, indent=4, index=False)\n",
        "\n",
        "            elif output_format == 'csv':\n",
        "                work_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "\n",
        "            else:\n",
        "                print(f\"경고: 지원하지 않는 형식 '{output_format}'입니다. 저장을 건너뜝니다.\")\n",
        "                break\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n{title} 작품 저장 중 오류 발생: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n총 {count}개 작품을 {output_format.upper()} 형식으로 {output_dir}/ 에 저장 완료.\")\n",
        "    return output_dir # 저장된 디렉토리 이름 반환\n",
        "\n",
        "\n",
        "def zip_directory(dir_name, zip_name):\n",
        "    \"\"\"지정된 디렉토리를 ZIP 파일로 압축합니다.\"\"\"\n",
        "    print(f\"\\n디렉토리 '{dir_name}'을 ZIP 파일 '{zip_name}'으로 압축 중...\")\n",
        "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(dir_name):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                # ZIP 파일 내 경로를 dir_name을 기준으로 상대 경로로 설정\n",
        "                zipf.write(file_path, os.path.relpath(file_path, os.path.dirname(dir_name)))\n",
        "    print(f\"압축 완료: {zip_name}\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2. 실행 및 ZIP 압축 (Colab용)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# 1. 분리할 DataFrame과 원하는 형식을 설정\n",
        "# (이전 셀에서 생성된 results_df 변수 사용)\n",
        "if 'results_df' in locals():\n",
        "    # 원하는 출력 형식을 여기서 선택하세요: 'xml', 'json', 'csv'\n",
        "    SPLIT_FORMAT = 'xml'\n",
        "    OUTPUT_DIR = f'works_split_{SPLIT_FORMAT}'\n",
        "    ZIP_FILENAME = f'{OUTPUT_DIR}.zip'\n",
        "\n",
        "    # 2. 개별 파일 저장 실행\n",
        "    saved_dir = split_and_save_by_work(\n",
        "        df=results_df,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        output_format=SPLIT_FORMAT\n",
        "    )\n",
        "\n",
        "    # 3. ZIP 압축 실행\n",
        "    zip_directory(saved_dir, ZIP_FILENAME)\n",
        "\n",
        "    # 4. Colab 환경에서 다운로드 링크 생성 (실제 다운로드를 위한 명령어)\n",
        "    from google.colab import files\n",
        "    print(f\"\\n다운로드 링크 생성 중...\")\n",
        "    # files.download(ZIP_FILENAME) # Colab 환경에서 다운로드 실행\n",
        "    print(f\"다운로드를 위해 'files.download(\\\"{ZIP_FILENAME}\\\")' 명령어를 실행하세요.\")\n",
        "\n",
        "else:\n",
        "    print(\"오류: 'results_df' DataFrame이 메모리에 없습니다. 이전 추출 셀을 실행했는지 확인하세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW3yHv93qIGI",
        "outputId": "796835dd-5478-4d6a-cec0-492c560c28a0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 디렉토리 사용: works_split_xml/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "개별 작품 (XML) 저장: 100%|██████████| 311/311 [00:01<00:00, 177.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "총 311개 작품을 XML 형식으로 works_split_xml/ 에 저장 완료.\n",
            "\n",
            "디렉토리 'works_split_xml'을 ZIP 파일 'works_split_xml.zip'으로 압축 중...\n",
            "압축 완료: works_split_xml.zip\n",
            "\n",
            "다운로드 링크 생성 중...\n",
            "다운로드를 위해 'files.download(\"works_split_xml.zip\")' 명령어를 실행하세요.\n"
          ]
        }
      ]
    }
  ]
}