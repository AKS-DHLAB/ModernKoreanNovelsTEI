{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 위키문헌 작품목록 텍스트 스크래핑\n",
        "- 작성자: [지해인](https://haein.info)"
      ],
      "metadata": {
        "id": "A2dhTZMJfjAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 환경 설정"
      ],
      "metadata": {
        "id": "DwJi9_Vyf_fq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hygwvzgqfg6e",
        "outputId": "5ac99bb0-97bb-49bc-d4b8-70ff808ce66f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mwxml\n",
            "  Downloading mwxml-0.3.6-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from mwxml) (4.25.1)\n",
            "Collecting mwcli>=0.0.2 (from mwxml)\n",
            "  Downloading mwcli-0.0.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting mwtypes>=0.4.0 (from mwxml)\n",
            "  Downloading mwtypes-0.4.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting para>=0.0.1 (from mwxml)\n",
            "  Downloading para-0.0.8-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (0.27.1)\n",
            "Collecting docopt (from mwcli>=0.0.2->mwxml)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonable>=0.3.0 (from mwtypes>=0.4.0->mwxml)\n",
            "  Downloading jsonable-0.3.1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=2.5.1->mwxml) (4.15.0)\n",
            "Downloading mwxml-0.3.6-py2.py3-none-any.whl (33 kB)\n",
            "Downloading mwcli-0.0.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Downloading mwtypes-0.4.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading para-0.0.8-py3-none-any.whl (6.5 kB)\n",
            "Downloading jsonable-0.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=1d0c6741273f61c1651bea74bcdab4f7169df178bef986f954936b7086029db7\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "Successfully built docopt\n",
            "Installing collected packages: para, jsonable, docopt, mwtypes, mwcli, mwxml\n",
            "Successfully installed docopt-0.6.2 jsonable-0.3.1 mwcli-0.0.3 mwtypes-0.4.0 mwxml-0.3.6 para-0.0.8\n",
            "모든 라이브러리가 성공적으로 로드되었습니다!\n",
            "사용 가능한 CPU 코어: 2개\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install mwxml requests tqdm pandas\n",
        "\n",
        "# 기본 라이브러리 임포트\n",
        "import mwxml\n",
        "import bz2\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import urllib.parse\n",
        "import multiprocessing as mp\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "print(\"모든 라이브러리가 성공적으로 로드되었습니다!\")\n",
        "print(f\"사용 가능한 CPU 코어: {mp.cpu_count()}개\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 다운로드"
      ],
      "metadata": {
        "id": "0U0nxEYpf8uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 위키문헌 덤프 다운로드\n",
        "dump_url = \"https://dumps.wikimedia.org/kowikisource/20251001/kowikisource-20251001-pages-articles.xml.bz2\"\n",
        "dump_file = \"kowikisource-20251001-pages-articles.xml.bz2\"\n",
        "\n",
        "if not os.path.exists(dump_file):\n",
        "    print(\" 위키문헌 덤프 다운로드 중... (약 150MB, 시간이 걸릴 수 있습니다)\")\n",
        "    !wget -O {dump_file} {dump_url}\n",
        "    print(\"다운로드 완료!\")\n",
        "else:\n",
        "    print(\"덤프 파일이 이미 존재합니다!\")\n",
        "\n",
        "# 파일 크기 확인\n",
        "file_size = os.path.getsize(dump_file) / (1024 * 1024)  # MB\n",
        "print(f\"파일 크기: {file_size:.1f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gkYrWrafoG7",
        "outputId": "981e28a1-d3ee-4018-83ac-de9dccd8ae14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 위키문헌 덤프 다운로드 중... (약 150MB, 시간이 걸릴 수 있습니다)\n",
            "--2025-10-20 12:39:00--  https://dumps.wikimedia.org/kowikisource/20251001/kowikisource-20251001-pages-articles.xml.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.71, 2620:0:861:3:208:80:154:71\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 155556940 (148M) [application/octet-stream]\n",
            "Saving to: ‘kowikisource-20251001-pages-articles.xml.bz2’\n",
            "\n",
            "kowikisource-202510 100%[===================>] 148.35M  3.40MB/s    in 36s     \n",
            "\n",
            "2025-10-20 12:39:37 (4.07 MB/s) - ‘kowikisource-20251001-pages-articles.xml.bz2’ saved [155556940/155556940]\n",
            "\n",
            "다운로드 완료!\n",
            "파일 크기: 148.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API 보강 함수"
      ],
      "metadata": {
        "id": "dvEvHbjFf4ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_categories_from_api(page_title):\n",
        "    \"\"\"\n",
        "    위키문헌 API에서 페이지의 모든 분류를 가져옵니다\n",
        "\n",
        "    이 함수가 중요한 이유:\n",
        "    - XML 덤프에는 기본 분류만 있음\n",
        "    - 템플릿에서 자동 생성되는 분류는 API로만 확인 가능\n",
        "    - 예: '1941년 작품', 'PD-old-50' 등\n",
        "    \"\"\"\n",
        "    try:\n",
        "        api_url = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'categories',\n",
        "            'cllimit': 'max'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(api_url, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return []\n",
        "\n",
        "        categories = pages[page_id].get('categories', [])\n",
        "        category_names = []\n",
        "\n",
        "        for cat in categories:\n",
        "            cat_title = cat['title']\n",
        "            if cat_title.startswith('분류:'):\n",
        "                category_names.append(cat_title[3:])  # '분류:' 제거\n",
        "\n",
        "        return category_names\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"API 분류 조회 오류 ({page_title}): {e}\")\n",
        "        return []\n",
        "\n",
        "def get_year_from_wikidata(page_title):\n",
        "    \"\"\"\n",
        "    위키데이터에서 작품의 발표 연도를 가져옵니다\n",
        "\n",
        "    위키데이터 연동의 장점:\n",
        "    - 구조화된 연도 정보 제공\n",
        "    - 여러 언어판에서 공유되는 정확한 데이터\n",
        "    - 분류 정보와 교차 검증 가능\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1단계: 위키문헌 페이지에서 위키데이터 ID 가져오기\n",
        "        wikisource_api = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'pageprops'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(wikisource_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return None\n",
        "\n",
        "        wikidata_id = pages[page_id].get('pageprops', {}).get('wikibase_item')\n",
        "\n",
        "        if not wikidata_id:\n",
        "            return None\n",
        "\n",
        "        # 2단계: 위키데이터에서 발표일 정보 가져오기\n",
        "        wikidata_api = \"https://www.wikidata.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'format': 'json',\n",
        "            'ids': wikidata_id,\n",
        "            'props': 'claims'\n",
        "        }\n",
        "\n",
        "        response = requests.get(wikidata_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        entity = data.get('entities', {}).get(wikidata_id, {})\n",
        "        claims = entity.get('claims', {})\n",
        "\n",
        "        # 발표 관련 속성들 확인\n",
        "        date_properties = ['P577', 'P571', 'P585']  # 발표일, 시작일, 특정시점\n",
        "\n",
        "        for prop in date_properties:\n",
        "            if prop in claims:\n",
        "                for claim in claims[prop]:\n",
        "                    try:\n",
        "                        time_value = claim['mainsnak']['datavalue']['value']['time']\n",
        "                        # +1941-00-00T00:00:00Z 형태에서 연도 추출\n",
        "                        year = int(time_value[1:5])\n",
        "                        if 1800 <= year <= 2030:  # 유효한 연도 범위\n",
        "                            return year\n",
        "                    except (KeyError, ValueError):\n",
        "                        continue\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"위키데이터 조회 오류 ({page_title}): {e}\")\n",
        "        return None\n",
        "\n",
        "def enhance_with_api(page_data):\n",
        "    \"\"\"\n",
        "    페이지 데이터를 API 정보로 보강합니다\n",
        "\n",
        "    보강 과정:\n",
        "    1. API에서 완전한 분류 정보 가져오기\n",
        "    2. 위키데이터에서 연도 정보 가져오기\n",
        "    3. 분류에서 연도 추출하기\n",
        "    4. 최종 연도 결정 (분류 > 위키데이터 > 덤프)\n",
        "    \"\"\"\n",
        "    title = page_data['title']\n",
        "\n",
        "    # API에서 완전한 분류 정보 가져오기\n",
        "    api_categories = get_categories_from_api(title)\n",
        "\n",
        "    # 위키데이터에서 연도 정보 가져오기\n",
        "    wikidata_year = get_year_from_wikidata(title)\n",
        "\n",
        "    # 기존 데이터 복사\n",
        "    enhanced = page_data.copy()\n",
        "\n",
        "    # 분류 정보 병합 (중복 제거)\n",
        "    all_categories = list(set(page_data.get('categories', []) + api_categories))\n",
        "    enhanced['categories'] = all_categories\n",
        "    enhanced['api_categories'] = api_categories\n",
        "\n",
        "    # 분류에서 연도 추출\n",
        "    year_from_categories = None\n",
        "    year_categories = [cat for cat in all_categories if '년' in cat and '작품' in cat]\n",
        "    if year_categories:\n",
        "        for cat in year_categories:\n",
        "            year_match = re.search(r'(\\d{4})년', cat)\n",
        "            if year_match:\n",
        "                year_from_categories = int(year_match.group(1))\n",
        "                break\n",
        "\n",
        "    # 최종 연도 결정 (우선순위: 분류 > 위키데이터 > 덤프)\n",
        "    enhanced['year'] = (\n",
        "        year_from_categories or\n",
        "        wikidata_year or\n",
        "        page_data.get('year')\n",
        "    )\n",
        "\n",
        "    # 보강 정보 추가\n",
        "    enhanced['year_from_categories'] = year_from_categories\n",
        "    enhanced['year_from_wikidata'] = wikidata_year\n",
        "\n",
        "    return enhanced\n",
        "\n",
        "print(\"API 보강 함수들이 정의되었습니다!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo5Y4WRVft_a",
        "outputId": "f0b270d2-891b-4e46-9096-5ac34a006037"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API 보강 함수들이 정의되었습니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 파싱 함수"
      ],
      "metadata": {
        "id": "iF2hBHdQgKrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_metadata(text):\n",
        "    \"\"\"\n",
        "    위키텍스트에서 메타데이터를 추출합니다\n",
        "\n",
        "    추출하는 정보:\n",
        "    - 저자: [[저자:이름]] 패턴에서\n",
        "    - 분류: [[분류:이름]] 패턴에서\n",
        "    - 작곡가: '작곡' 키워드 주변에서\n",
        "    - 라이선스: {{PD-*}} 템플릿에서\n",
        "    - 언어: '한자', '한글' 키워드에서\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return {\n",
        "            'authors': [],\n",
        "            'categories': [],\n",
        "            'composer': None,\n",
        "            'translator': None,\n",
        "            'year': None,\n",
        "            'license': None,\n",
        "            'language': None\n",
        "        }\n",
        "\n",
        "    # 저자 정보 추출\n",
        "    authors = []\n",
        "    author_patterns = [\n",
        "        r'\\[\\[저자:([^|\\]]+)',  # [[저자:이름]] 또는 [[저자:이름|표시명]]\n",
        "        r'\\|\\s*author\\s*=\\s*\\[\\[저자:([^|\\]]+)',  # author= 매개변수\n",
        "    ]\n",
        "\n",
        "    for pattern in author_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        authors.extend(matches)\n",
        "\n",
        "    # 분류 정보 추출\n",
        "    categories = re.findall(r'\\[\\[분류:([^\\]]+)\\]\\]', text)\n",
        "\n",
        "    # 작곡가 정보 추출\n",
        "    composer = None\n",
        "    composer_patterns = [\n",
        "        r'([^.]+)\\s*작곡',\n",
        "        r'작곡가?\\s*[:=]\\s*([^.\\n]+)'\n",
        "    ]\n",
        "    for pattern in composer_patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        if matches:\n",
        "            composer = matches[0].strip()\n",
        "            break\n",
        "\n",
        "    # 연도 정보 추출\n",
        "    year_matches = re.findall(r'(\\d{4})년', text)\n",
        "    year = None\n",
        "    if year_matches:\n",
        "        year = max(set(year_matches), key=year_matches.count)\n",
        "\n",
        "    # 라이선스 정보 추출\n",
        "    license_info = None\n",
        "    license_patterns = [\n",
        "        r'\\{\\{(PD-[^}]+)\\}\\}',\n",
        "        r'\\{\\{(CC-[^}]+)\\}\\}'\n",
        "    ]\n",
        "    for pattern in license_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            license_info = matches[0]\n",
        "            break\n",
        "\n",
        "    # 언어 정보 추출\n",
        "    language = None\n",
        "    if '한자' in text and '한글' in text:\n",
        "        language = '한자+한글'\n",
        "    elif '한자' in text:\n",
        "        language = '한자'\n",
        "    elif '한글' in text:\n",
        "        language = '한글'\n",
        "\n",
        "    # 중복 제거\n",
        "    authors = list(set([a.strip() for a in authors if a.strip()]))\n",
        "    categories = list(set([c.strip() for c in categories if c.strip()]))\n",
        "\n",
        "    return {\n",
        "        'authors': authors,\n",
        "        'categories': categories,\n",
        "        'composer': composer,\n",
        "        'translator': None,  # 나중에 확장 가능\n",
        "        'year': year,\n",
        "        'license': license_info,\n",
        "        'language': language\n",
        "    }\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    위키텍스트에서 마크업을 제거하고 깔끔한 본문만 추출합니다\n",
        "\n",
        "    제거하는 것들:\n",
        "    - 템플릿: {{...}}\n",
        "    - HTML 태그: <div>, <br> 등\n",
        "    - 위키 링크: [[...]]\n",
        "    - 마크업: ''', ''\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # 템플릿 제거\n",
        "    cleaned = re.sub(r'\\{\\{[^{}]*\\}\\}', '', text)\n",
        "\n",
        "    # 링크에서 텍스트만 추출\n",
        "    cleaned = re.sub(r'\\[\\[[^|\\]]*\\|([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "    cleaned = re.sub(r'\\[\\[([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "\n",
        "    # HTML 태그 제거\n",
        "    cleaned = re.sub(r'<[^>]+>', '', cleaned)\n",
        "\n",
        "    # 위키 마크업 제거\n",
        "    cleaned = re.sub(r\"'''([^']+)'''\", r'\\1', cleaned)  # 굵은 글씨\n",
        "    cleaned = re.sub(r\"''([^']+)''\", r'\\1', cleaned)   # 기울임\n",
        "\n",
        "    # 섹션 헤더 제거\n",
        "    cleaned = re.sub(r'^=+\\s*([^=]+)\\s*=+$', r'\\1', cleaned, flags=re.MULTILINE)\n",
        "\n",
        "    # 여러 공백을 하나로\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_urls(title, authors):\n",
        "    \"\"\"\n",
        "    페이지와 저자의 URL을 생성합니다\n",
        "    \"\"\"\n",
        "    base_url = \"https://ko.wikisource.org/wiki/\"\n",
        "\n",
        "    # 페이지 URL\n",
        "    page_url = base_url + urllib.parse.quote(title.replace(' ', '_'))\n",
        "\n",
        "    # 저자 URL들\n",
        "    author_links = []\n",
        "    for author in authors:\n",
        "        author_url = base_url + urllib.parse.quote(f\"저자:{author}\".replace(' ', '_'))\n",
        "        author_links.append({\n",
        "            'name': author,\n",
        "            'url': author_url\n",
        "        })\n",
        "\n",
        "    return page_url, author_links\n",
        "\n",
        "print(\"텍스트 파싱 함수들이 정의되었습니다!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hCokgZyf7Gq",
        "outputId": "05ec3576-d329-4dfe-9f21-c4bba45e2d95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "텍스트 파싱 함수들이 정의되었습니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 메인 파서"
      ],
      "metadata": {
        "id": "UPgSc7v1gVyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pages_batch(pages_batch):\n",
        "    \"\"\"\n",
        "    페이지 배치를 처리하는 워커 함수\n",
        "    (멀티프로세싱에서 각 코어가 실행)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for page_data in pages_batch:\n",
        "        try:\n",
        "            page_id, title, namespace, redirect, revisions = page_data\n",
        "\n",
        "            if not revisions:\n",
        "                continue\n",
        "\n",
        "            # 최신 리비전 사용\n",
        "            revision = revisions[0]\n",
        "            revision_id, timestamp, username, comment, text, size = revision\n",
        "\n",
        "            # 메타데이터 추출\n",
        "            metadata = extract_metadata(text)\n",
        "\n",
        "            # 본문 정리\n",
        "            clean_content = clean_text(text)\n",
        "\n",
        "            # URL 생성\n",
        "            page_url, author_links = generate_urls(title, metadata['authors'])\n",
        "\n",
        "            # 페이지 데이터 구성\n",
        "            page_result = {\n",
        "                'page_id': page_id,\n",
        "                'title': title,\n",
        "                'url': page_url,\n",
        "                'namespace': namespace,\n",
        "                'redirect': redirect,\n",
        "\n",
        "                # 필수 메타데이터\n",
        "                'authors': metadata['authors'],\n",
        "                'author_links': author_links,\n",
        "                'categories': metadata['categories'],\n",
        "                'content': clean_content,\n",
        "                'raw_content': text,\n",
        "\n",
        "                # 추가 메타데이터\n",
        "                'composer': metadata['composer'],\n",
        "                'translator': metadata['translator'],\n",
        "                'year': metadata['year'],\n",
        "                'license': metadata['license'],\n",
        "                'language': metadata['language'],\n",
        "\n",
        "                # 리비전 정보\n",
        "                'revision_id': revision_id,\n",
        "                'last_modified': str(timestamp) if timestamp else None,\n",
        "                'last_contributor': username,\n",
        "                'size': len(text) if text else 0,\n",
        "                'content_size': len(clean_content)\n",
        "            }\n",
        "\n",
        "            results.append(page_result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"페이지 처리 오류: {e}\")\n",
        "            continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def parse_wikisource(dump_file, limit=None, enable_api=False, batch_size=50):\n",
        "    \"\"\"\n",
        "    위키문헌 덤프를 파싱합니다\n",
        "\n",
        "    Args:\n",
        "        dump_file: 덤프 파일 경로\n",
        "        limit: 처리할 페이지 수 제한 (None이면 전체)\n",
        "        enable_api: API 보강 사용 여부\n",
        "        batch_size: 배치 크기\n",
        "\n",
        "    Returns:\n",
        "        list: 파싱된 페이지 데이터\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # CPU 코어 수 (Colab에서 최대 활용)\n",
        "    max_workers = mp.cpu_count()\n",
        "\n",
        "    print(f\" 위키문헌 파싱 시작!\")\n",
        "    print(f\"   CPU 코어: {max_workers}개 (최대 활용)\")\n",
        "    print(f\"   배치 크기: {batch_size}\")\n",
        "    print(f\"   API 보강: {'사용' if enable_api else '사용 안함'}\")\n",
        "\n",
        "    # 1단계: 덤프에서 데이터 수집\n",
        "    print(\"\\n 1단계: 덤프 데이터 수집\")\n",
        "    pages_batches = []\n",
        "    current_batch = []\n",
        "    total_pages = 0\n",
        "\n",
        "    with bz2.open(dump_file, 'rt', encoding='utf-8') as f:\n",
        "        dump = mwxml.Dump.from_file(f)\n",
        "\n",
        "        for page in tqdm(dump, desc=\"페이지 수집\"):\n",
        "            if limit and total_pages >= limit:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # 리비전 데이터 수집\n",
        "                revisions = []\n",
        "                for revision in page:\n",
        "                    revisions.append((\n",
        "                        revision.id,\n",
        "                        revision.timestamp,\n",
        "                        revision.user.text if revision.user else None,\n",
        "                        revision.comment,\n",
        "                        revision.text,\n",
        "                        revision.bytes\n",
        "                    ))\n",
        "                    break  # 최신 리비전만\n",
        "\n",
        "                page_data = (\n",
        "                    page.id,\n",
        "                    page.title,\n",
        "                    page.namespace,\n",
        "                    str(page.redirect.title) if page.redirect else None,\n",
        "                    revisions\n",
        "                )\n",
        "                current_batch.append(page_data)\n",
        "\n",
        "                if len(current_batch) >= batch_size:\n",
        "                    pages_batches.append(current_batch)\n",
        "                    current_batch = []\n",
        "\n",
        "                total_pages += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if current_batch:\n",
        "            pages_batches.append(current_batch)\n",
        "\n",
        "    print(f\" {total_pages}개 페이지를 {len(pages_batches)}개 배치로 수집\")\n",
        "\n",
        "    # 2단계: 멀티프로세싱으로 병렬 처리\n",
        "    print(\"\\n 2단계: 멀티프로세싱 처리\")\n",
        "    results = []\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # 모든 배치를 병렬로 처리\n",
        "        futures = [executor.submit(process_pages_batch, batch) for batch in pages_batches]\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"배치 처리\"):\n",
        "            try:\n",
        "                batch_results = future.result()\n",
        "                results.extend(batch_results)\n",
        "            except Exception as e:\n",
        "                print(f\"배치 처리 오류: {e}\")\n",
        "\n",
        "    # 3단계: API 보강 (선택적)\n",
        "    if enable_api and results:\n",
        "        print(\"\\n 3단계: API 보강 처리\")\n",
        "        enhanced_results = []\n",
        "\n",
        "        for page_data in tqdm(results, desc=\"API 보강\"):\n",
        "            try:\n",
        "                enhanced = enhance_with_api(page_data)\n",
        "                enhanced_results.append(enhanced)\n",
        "                time.sleep(0.1)  # API 제한 고려\n",
        "            except Exception as e:\n",
        "                print(f\"API 보강 오류 ({page_data.get('title', 'Unknown')}): {e}\")\n",
        "                enhanced_results.append(page_data)\n",
        "\n",
        "        results = enhanced_results\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n파싱 완료!\")\n",
        "    print(f\"  총 시간: {total_time:.1f}초\")\n",
        "    print(f\"  처리 속도: {len(results)/total_time:.1f} 페이지/초\")\n",
        "    print(f\"  총 페이지: {len(results)}개\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"메인 파서가 정의되었습니다!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbbwBTwRgMo8",
        "outputId": "c8030663-7ad7-40de-de95-3facc12e51a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "메인 파서가 정의되었습니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 대량 파싱"
      ],
      "metadata": {
        "id": "6EZlLpCIgncB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 대량 처리 실행\n",
        "print(\" 실습 3: 대량 처리\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 처리할 페이지 수 설정 (필요에 따라 조정)\n",
        "BULK_LIMIT = 50  # 50개 페이지 처리\n",
        "API_ENHANCEMENT = True  # API 보강 사용 여부\n",
        "\n",
        "print(f\" 설정:\")\n",
        "print(f\"  처리 페이지: {BULK_LIMIT}개\")\n",
        "print(f\"  API 보강: {'사용' if API_ENHANCEMENT else '사용 안함'}\")\n",
        "print(f\"  CPU 활용: {mp.cpu_count()}개 코어 최대 활용\")\n",
        "\n",
        "# 대량 처리 실행\n",
        "bulk_results = parse_wikisource(\n",
        "    dump_file,\n",
        "    limit=BULK_LIMIT,\n",
        "    enable_api=API_ENHANCEMENT,\n",
        "    batch_size=10  # 배치 크기\n",
        ")\n",
        "\n",
        "print(f\"\\n 대량 처리 완료!\")\n",
        "print(f\"처리된 페이지: {len(bulk_results)}개\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWlmjWXcgf4i",
        "outputId": "e0ac2f3d-78a3-44dc-b026-e79ec98cf6d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 실습 3: 대량 처리\n",
            "==================================================\n",
            " 설정:\n",
            "  처리 페이지: 50개\n",
            "  API 보강: 사용\n",
            "  CPU 활용: 2개 코어 최대 활용\n",
            " 위키문헌 파싱 시작!\n",
            "   CPU 코어: 2개 (최대 활용)\n",
            "   배치 크기: 10\n",
            "   API 보강: 사용\n",
            "\n",
            " 1단계: 덤프 데이터 수집\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "페이지 수집: 50it [00:00, 220.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 50개 페이지를 5개 배치로 수집\n",
            "\n",
            " 2단계: 멀티프로세싱 처리\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "배치 처리: 100%|██████████| 5/5 [00:11<00:00,  2.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 3단계: API 보강 처리\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "API 보강: 100%|██████████| 50/50 [00:20<00:00,  2.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "파싱 완료!\n",
            "  총 시간: 33.0초\n",
            "  처리 속도: 1.5 페이지/초\n",
            "  총 페이지: 50개\n",
            "\n",
            " 대량 처리 완료!\n",
            "처리된 페이지: 50개\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 개별 문서 불러오기"
      ],
      "metadata": {
        "id": "z8eLU1j4iNdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 링크로 최신 문서 불러오기"
      ],
      "metadata": {
        "id": "Q_PpMf2ImMh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikitext_from_api(page_title):\n",
        "    \"\"\"\n",
        "    위키문헌 API를 사용하여 페이지의 위키텍스트(raw content)를 가져옴\n",
        "    \"\"\"\n",
        "    try:\n",
        "        api_url = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'revisions',\n",
        "            'rvprop': 'content|ids|timestamp|user|size|comment',\n",
        "            'rvslots': 'main',\n",
        "            'rvlimit': '1'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(api_url, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return None, None # 텍스트 없음, 리비전 정보 없음\n",
        "\n",
        "        revisions = pages[page_id].get('revisions', [])\n",
        "        if not revisions:\n",
        "            return None, None\n",
        "\n",
        "        revision = revisions[0]\n",
        "        # 'content' 또는 '*' 필드에서 텍스트를 가져옴\n",
        "        text = revision.get('slots', {}).get('main', {}).get('*')\n",
        "\n",
        "        revision_info = {\n",
        "            'revision_id': revision.get('revid'),\n",
        "            'last_modified': revision.get('timestamp'),\n",
        "            'last_contributor': revision.get('user'),\n",
        "            'size': revision.get('size'),\n",
        "            'comment': revision.get('comment')\n",
        "        }\n",
        "\n",
        "        return text, revision_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"API 위키텍스트 조회 오류 ({page_title}): {e}\")\n",
        "        return None, None\n",
        "\n",
        "def extract_page_title(link_url):\n",
        "    \"\"\"\n",
        "    위키문헌 URL에서 페이지 제목을 추출하고 디코딩\n",
        "    예: 'https://ko.wikisource.org/wiki/혈의_누' -> '혈의 누'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # URL 디코딩\n",
        "        decoded_url = urllib.parse.unquote(link_url)\n",
        "        # '/wiki/' 뒤의 부분을 가져와서 밑줄을 공백으로 변환\n",
        "        title_with_underscores = decoded_url.split('/wiki/')[1]\n",
        "        title = title_with_underscores.replace('_', ' ')\n",
        "        return title\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def process_csv_links_with_api(csv_file='한국근대소설_TEI_XML_작품목록.csv'):\n",
        "    \"\"\"\n",
        "    CSV 파일에서 링크를 읽고 API를 통해 위키문헌 데이터를 추출 및 보강합니다.\n",
        "    \"\"\"\n",
        "    print(f\"\\n CSV 파일 로드: {csv_file}\")\n",
        "\n",
        "    # 1. CSV 파일 로드\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\" 오류: '{csv_file}' 파일을 찾을 수 없습니다. 파일을 업로드했는지 확인해주세요.\")\n",
        "        return []\n",
        "\n",
        "    if '링크' not in df.columns:\n",
        "        print(\" 오류: CSV 파일에 '링크' 열이 없습니다. 열 이름을 확인해주세요.\")\n",
        "        return []\n",
        "\n",
        "    valid_links = df['링크'].dropna().astype(str)\n",
        "    print(f\" 총 {len(valid_links)}개의 링크 발견. 데이터 추출 시작...\")\n",
        "\n",
        "    final_data = []\n",
        "\n",
        "    # tqdm을 사용하여 진행 상황 표시\n",
        "    for index, link_url in tqdm(valid_links.items(), total=len(valid_links), desc=\"작품별 API 처리\"):\n",
        "        if not link_url.startswith('https://ko.wikisource.org/wiki/'):\n",
        "            continue\n",
        "\n",
        "        page_title = extract_page_title(link_url)\n",
        "\n",
        "        if not page_title:\n",
        "            continue\n",
        "\n",
        "        # 2. 위키텍스트 및 기본 정보 API로 가져오기\n",
        "        raw_content, revision_info = get_wikitext_from_api(page_title)\n",
        "\n",
        "        if not raw_content:\n",
        "            continue\n",
        "\n",
        "        # 3. 기존 정의된 함수들로 메타데이터 추출 및 본문 정리\n",
        "        metadata = extract_metadata(raw_content)\n",
        "        clean_content = clean_text(raw_content)\n",
        "        page_url, author_links = generate_urls(page_title, metadata['authors'])\n",
        "\n",
        "        # 기본 데이터 구조 생성\n",
        "        page_data = {\n",
        "            'page_id': revision_info.get('revision_id', None),\n",
        "            'title': page_title,\n",
        "            'url': page_url,\n",
        "            'namespace': 0,\n",
        "            'redirect': None,\n",
        "\n",
        "            # 메타데이터\n",
        "            'authors': metadata['authors'],\n",
        "            'author_links': author_links,\n",
        "            'categories': metadata['categories'],\n",
        "            'content': clean_content,\n",
        "            'raw_content': raw_content,\n",
        "\n",
        "            'composer': metadata['composer'],\n",
        "            'translator': metadata['translator'],\n",
        "            'year': metadata['year'], # 덤프 파싱으로 추출된 year (API 보강 후 최종 year 결정)\n",
        "            'license': metadata['license'],\n",
        "            'language': metadata['language'],\n",
        "\n",
        "            # 리비전 정보\n",
        "            'revision_id': revision_info.get('revision_id'),\n",
        "            'last_modified': revision_info.get('last_modified'),\n",
        "            'last_contributor': revision_info.get('last_contributor'),\n",
        "            'size': len(raw_content) if raw_content else 0,\n",
        "            'content_size': len(clean_content)\n",
        "        }\n",
        "\n",
        "        # 4. API 보강 (완전한 분류, 위키데이터 연도 등)\n",
        "        enhanced_data = enhance_with_api(page_data)\n",
        "        final_data.append(enhanced_data)\n",
        "\n",
        "        # API 요청 제한을 위한 대기 (매우 중요)\n",
        "        time.sleep(0.3)\n",
        "\n",
        "    print(f\"\\n 총 {len(final_data)}개의 작품 데이터 추출 및 보강 완료.\")\n",
        "\n",
        "    return final_data\n",
        "\n",
        "print(\"API 호출 및 제목 추출 등을 위한 함수 정의 완료.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6zKcKP1grpS",
        "outputId": "0c66282a-cfc4-461a-8fb4-97cab7e46a7b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API 호출 및 제목 추출 등을 위한 함수 정의 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DF to XML"
      ],
      "metadata": {
        "id": "HDvA-BMynf19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom import minidom\n",
        "\n",
        "def dataframe_to_xml(df):\n",
        "    \"\"\"\n",
        "    Pandas DataFrame을 지정된 포맷의 XML 문자열로 변환합니다.\n",
        "    \"\"\"\n",
        "    root = ET.Element('works')\n",
        "\n",
        "    # NaN 값을 빈 문자열로 처리하여 XML 변환 시 오류를 방지합니다.\n",
        "    df = df.fillna('')\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        work = ET.SubElement(root, 'work')\n",
        "\n",
        "        # 'title', 'url', 'authors', 'year' 등 주요 필드를 태그로 추가\n",
        "        for col in ['title', 'url', 'year', 'license']:\n",
        "            elem = ET.SubElement(work, col)\n",
        "            elem.text = str(row[col])\n",
        "\n",
        "        # authors 목록 처리\n",
        "        authors_elem = ET.SubElement(work, 'authors')\n",
        "        if isinstance(row['authors'], list):\n",
        "             for author_name in row['authors']:\n",
        "                author_elem = ET.SubElement(authors_elem, 'author')\n",
        "                author_elem.text = str(author_name)\n",
        "\n",
        "        # categories 목록 처리\n",
        "        categories_elem = ET.SubElement(work, 'categories')\n",
        "        if isinstance(row['categories'], list):\n",
        "            for cat_name in row['categories']:\n",
        "                cat_elem = ET.SubElement(categories_elem, 'category')\n",
        "                cat_elem.text = str(cat_name)\n",
        "\n",
        "        # 'content' (깔끔한 본문) 처리\n",
        "        content_elem = ET.SubElement(work, 'content')\n",
        "        content_elem.text = row['content']\n",
        "\n",
        "        # 주석 정보 (리비전 정보 등)는 속성으로 추가\n",
        "        work.set('page_id', str(row['page_id']))\n",
        "        work.set('revision_id', str(row['revision_id']))\n",
        "\n",
        "    # Pretty Print (들여쓰기)를 적용하여 사람이 읽기 쉽게 만듭니다.\n",
        "    xml_string = ET.tostring(root, encoding='utf-8')\n",
        "    reparsed = minidom.parseString(xml_string)\n",
        "    return reparsed.toprettyxml(indent=\"  \", encoding='utf-8')\n",
        "\n",
        "print(\"XML 변환 헬퍼 함수 정의 완료.\")\n",
        "\n",
        "def output_data(df, base_filename='extracted_wikisource_modern_novels', output_format='json'):\n",
        "    \"\"\"\n",
        "    DataFrame을 지정된 형식(json, csv, tsv, xml)으로 저장합니다.\n",
        "    \"\"\"\n",
        "    output_format = output_format.lower()\n",
        "    output_filename = f\"{base_filename}.{output_format}\"\n",
        "\n",
        "    print(f\"\\n파일 형식: {output_format.upper()}로 저장 중...\")\n",
        "\n",
        "    try:\n",
        "        if output_format == 'json':\n",
        "            df.to_json(output_filename, orient='records', force_ascii=False, indent=4)\n",
        "        elif output_format == 'csv':\n",
        "            df.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "        elif output_format == 'tsv':\n",
        "            df.to_csv(output_filename, index=False, sep='\\t', encoding='utf-8')\n",
        "        elif output_format == 'xml':\n",
        "            # 수동 XML 변환 함수 사용\n",
        "            xml_output = dataframe_to_xml(df)\n",
        "            with open(output_filename, 'wb') as f:\n",
        "                f.write(xml_output)\n",
        "        else:\n",
        "            print(f\"경고: 지원하지 않는 형식 '{output_format}'. JSON으로 저장합니다.\")\n",
        "            output_filename = f\"{base_filename}.json\"\n",
        "            df.to_json(output_filename, orient='records', force_ascii=False, indent=4)\n",
        "\n",
        "        print(f\" 데이터가 '{output_filename}'으로 저장되었습니다. 다운로드하여 확인하세요.\")\n",
        "    except Exception as e:\n",
        "        print(f\" 데이터 저장 오류 ({output_format}): {e}\")\n",
        "        # XML 저장이 실패하면, CSV로 대체 저장하는 옵션도 고려할 수 있습니다.\n",
        "        if output_format == 'xml':\n",
        "             print(\"대체: XML 저장에 실패하여 CSV로 저장합니다.\")\n",
        "             df.to_csv(f\"{base_filename}_fallback.csv\", index=False, encoding='utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZumYKov0moum",
        "outputId": "5343f47f-8502-451b-8211-1deecc927f6d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XML 변환 헬퍼 함수 정의 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 데이터 추출\n",
        "extracted_works = process_csv_links_with_api()\n",
        "\n",
        "# 2. 결과 처리 및 저장\n",
        "if extracted_works:\n",
        "    results_df = pd.DataFrame(extracted_works)\n",
        "    print(\"\\n 추출된 데이터 요약 (상위 5개)\")\n",
        "    print(results_df[['title', 'authors', 'year', 'year_from_wikidata', 'year_from_categories', 'license', 'url']].head())\n",
        "\n",
        "\n",
        "    # 형식 선택: 'json', 'csv', 'tsv', 'xml' 중 하나를 선택\n",
        "    desired_format = 'xml' # XML로 선택하여 수동 변환 테스트\n",
        "\n",
        "    output_data(results_df, output_format=desired_format)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eBIlmbwniFV",
        "outputId": "96ef358a-f200-4359-b06c-963177e4ca4e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " CSV 파일 로드: 한국근대소설_TEI_XML_작품목록.csv\n",
            " 총 42개의 링크 발견. 데이터 추출 시작...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "작품별 API 처리: 100%|██████████| 42/42 [00:51<00:00,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 총 42개의 작품 데이터 추출 및 보강 완료.\n",
            "\n",
            " 추출된 데이터 요약 (상위 5개)\n",
            "  title      authors  year  year_from_wikidata  year_from_categories  \\\n",
            "0  혈의 누        [이인직]  1906              1906.0                1906.0   \n",
            "1   철세계  [쥘 베른, 이해조]  None                 NaN                   NaN   \n",
            "2   자유종        [이해조]  1910                 NaN                1910.0   \n",
            "3  화의 혈        [이해조]  1911                 NaN                1911.0   \n",
            "4   추월색        [최찬식]  1912                 NaN                1912.0   \n",
            "\n",
            "           license                                                url  \n",
            "0       PD-old-100  https://ko.wikisource.org/wiki/%ED%98%88%EC%9D...  \n",
            "1       PD-old-100  https://ko.wikisource.org/wiki/%EC%B2%A0%EC%84...  \n",
            "2  PD-US|1927|1910  https://ko.wikisource.org/wiki/%EC%9E%90%EC%9C...  \n",
            "3        PD-old-70  https://ko.wikisource.org/wiki/%ED%99%94%EC%9D...  \n",
            "4       PD-old-100  https://ko.wikisource.org/wiki/%EC%B6%94%EC%9B...  \n",
            "\n",
            "파일 형식: XML로 저장 중...\n",
            " 데이터가 'extracted_wikisource_modern_novels.xml'으로 저장되었습니다. 다운로드하여 확인하세요.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}